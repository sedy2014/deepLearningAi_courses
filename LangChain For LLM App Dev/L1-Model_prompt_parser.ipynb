{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: LangChain - Models, Prompts, and Output Parsers\n",
    "\n",
    "This tutorial provides a comprehensive guide to using **LangChain**, a framework that simplifies the development of applications powered by large language models (LLMs)[2][3][4].\n",
    "\n",
    "In this notebook, you will learn the following:\n",
    "\n",
    "- How to set up your environment (installing required packages and setting API keys).\n",
    "- How to perform direct API calls to OpenAI using a custom function.\n",
    "- How to leverage LangChain’s higher-level abstractions to build chat models with prompt templates.\n",
    "- How to use output parsers to convert raw LLM text responses into structured JSON data.\n",
    "\n",
    "LangChain is a modular framework that allows developers to combine language models with external data sources and processing pipelines efficiently, saving time and reducing complexity in application development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before you begin, make sure you have the required packages installed and that your OpenAI API key is properly configured. The following steps load environment variables from a `.env` file and install packages if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment the following lines to install required packages if they are not already installed.\n",
    "# !pip install python-dotenv\n",
    "# !pip install openai\n",
    "# !pip install langchain"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries and load environment variables\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# This loads variables from your local .env file into the environment\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set the OpenAI API key from the environment variable. Make sure your .env file contains the OPENAI_API_KEY\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct API Call to OpenAI\n",
    "\n",
    "The following section defines a function `get_completion` that makes a direct API call to OpenAI’s **ChatCompletion** endpoint. This function takes a `prompt` and optionally a `model` argument. It uses a deterministic `temperature` (set to 0) so that the output remains consistent every time.\n",
    "\n",
    "The model usage is controlled by the current date. If the date is past a specified target date, it uses the latest model version (\"gpt-3.5-turbo\"); otherwise, it uses an older snapshot (\"gpt-3.5-turbo-0301\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Specify the target date for updating the model\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Select the model based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "def get_completion(prompt, model=llm_model):\n",
    "    \"\"\"\n",
    "    Send a prompt to the OpenAI ChatCompletion API and return the model's response.\n",
    "\n",
    "    Arguments:\n",
    "      prompt (str): The text prompt to send to the model.\n",
    "      model (str): The name of the model to use (default is controlled by the current date).\n",
    "    \"\"\"\n",
    "    # Structure the message as required by the API: role and content\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Call OpenAI's ChatCompletion API with specified parameters\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # Lower temperature for deterministic responses\n",
    "    )\n",
    "    \n",
    "    # Return the text content of the first response\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# Example of using the get_completion function\n",
    "print(get_completion(\"What is 1+1?\"))  # Expected output is '2' or a similar deterministic answer"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain's Chat API\n",
    "\n",
    "LangChain provides abstraction layers that make it easier to work with language models. In this section, we use the `ChatOpenAI` class from LangChain to create a chat model. By doing so, you don’t have to manage the low-level API details directly.\n",
    "\n",
    "We also demonstrate how to build a prompt template with placeholders (e.g., `{style}` and `{text}`) using the `ChatPromptTemplate` class. This template can dynamically format messages before sending them to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create a ChatOpenAI instance with no randomness (temperature=0.0) and the chosen model\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "print(chat)  # This displays information about the chat model instance"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a prompt template that instructs the model to translate text into a specific style\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ``````\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template object from the template string\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "# Verify the template's content to ensure it is set up correctly\n",
    "print(prompt_template.messages[0].prompt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define sample input text (in this case, a customer email written in a pirate style)\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "# Define the target style for translation (American English, calm and respectful)\n",
    "customer_style = \"\"\"\n",
    "American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "# Format the prompt message by inserting the customer's email and desired style into the template\n",
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style,\n",
    "    text=customer_email\n",
    ")\n",
    "\n",
    "print(type(customer_messages))  # Should output a list type\n",
    "print(customer_messages[0])        # Displays the formatted message\n",
    "\n",
    "# Call the chat model with the formatted prompt and print the translated response\n",
    "customer_response = chat(customer_messages)\n",
    "print(customer_response.content)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers in LangChain\n",
    "\n",
    "Output parsers are used to convert raw textual responses from an LLM into structured data such as JSON, which is helpful when expecting specific output formats.\n",
    "\n",
    "The following code uses LangChain’s `ResponseSchema` and `StructuredOutputParser` classes. It defines a response schema with three keys:\n",
    "\n",
    "- **gift**: Indicates if an item was purchased as a gift.\n",
    "- **delivery_days**: The number of days it took for delivery (or -1 if unknown).\n",
    "- **price_value**: Sentences about the pricing or value of an item, returned as a comma-separated list.\n",
    "\n",
    "After defining the schemas, the notebook creates a format instruction string which is then appended to a prompt template. The final response from the LLM is parsed into a Python dictionary for easy use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# Define the expected structured output schemas\n",
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\"\n",
    ")\n",
    "\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\"\n",
    ")\n",
    "\n",
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any sentences about the value or price, and output them as a comma separated Python list.\"\n",
    ")\n",
    "\n",
    "# Combine the schemas into a list\n",
    "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n",
    "\n",
    "# Build a StructuredOutputParser from the response schemas\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Get instructions on how to format the output (this will be appended to the prompt for guidance)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define an example review text that describes product features and delivery details\n",
    "customer_review = \"\"\"\n",
    "This leaf blower is pretty amazing. It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "# Create a review prompt template using the expected output instructions\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a ChatPromptTemplate using the review template\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template)\n",
    "\n",
    "# Format the message with the review text and the format instructions\n",
    "messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)\n",
    "\n",
    "# Call the chat model to obtain and print the raw response\n",
    "response = chat(messages)\n",
    "print(response.content)\n",
    "\n",
    "# Parse the response into a structured Python dictionary\n",
    "output_dict = output_parser.parse(response.content)\n",
    "\n",
    "# Print the parsed output\n",
    "print(output_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have demonstrated:\n",
    "\n",
    "- **Direct API calls** to OpenAI to generate text responses.\n",
    "- Using **LangChain’s Chat API** to create chat models and format dynamic prompts.\n",
    "- Leveraging the **Prompt Template** module to insert variables into text templates.\n",
    "- Utilizing **Output Parsers** to convert raw text responses into structured JSON data.\n",
    "\n",
    "LangChain is a valuable toolkit for AI developers because it abstracts many of the complexities involved in building, testing, and deploying language model applications.\n",
    "\n",
    "Happy coding and exploration with LangChain!"
   ]
  }
 ]
}
