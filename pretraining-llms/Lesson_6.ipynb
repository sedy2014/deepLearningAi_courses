{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2a8fff-4124-4c6d-9258-c05657ec01c8",
   "metadata": {
    "id": "ae2a8fff-4124-4c6d-9258-c05657ec01c8"
   },
   "source": [
    "# Lesson 6. Model evaluation\n",
    "\n",
    "## Introduction\n",
    "* Evaluating language models (LLMs) is a critical step in determining their effectiveness in different tasks.\n",
    "This notebook introduces tools for LLM evaluation, such as **LM Evaluation Harness**, and explains how to test models using **TruthfulQA MC2** and the **Hugging Face Leaderboard**.\n",
    "\n",
    "* The model comparison tool  described in the video can be found at this link: https://console.upstage.ai/ (note that you need to create a free account to try it out.)\n",
    "\n",
    "Information about the harness can be found at this [github repo](https://github.com/EleutherAI/lm-evaluation-harness):\n",
    "\n",
    "There are 4 steps in model evaluation:\n",
    "*  **Look at Loss**  : Training loss should decrease with epochs.\n",
    "*  **Check Model Ops during  training**: create model checkpoints periodiically\n",
    "*  **Compare with other models** : using online tools\n",
    "*  **Using LLM model as judge to compare models**\n",
    "\n",
    "#### Workflow\n",
    "In this lesson, we focus on evaluating pretrained language models \n",
    "\n",
    "### Key Objectives:\n",
    "1.  **Model Initialization**\n",
    "   - Load pretrained models using Hugging Face's `AutoModelForCausalLM` or `LlamaForCausalLM`.  \n",
    "2.  **Evaluation Workflow**\n",
    "   - Perform log-likelihood evaluations on datasets to assess model performance.\n",
    "   - Use metrics such as accuracy (`acc`) and log-likelihood scores to evaluate the model's ability to generate or predict text.\n",
    "\n",
    "3.  **Practical Debugging Tips**\n",
    "   - Use `nvidia-smi` to monitor GPU utilization during evaluation.\n",
    "   - Optimize batch size and precision (`torch_dtype=torch.bfloat16`) for faster inference.\n",
    "\n",
    "**Next Steps**:\n",
    "- Fine-tune models on specific datasets to improve task-specific performance.\n",
    "- Experiment with different precision settings (FP16, BF16) and batch sizes to optimize throughput during evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edebe7-ca5d-430d-ac40-9ec8dc2c2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fbf66-c7d5-4323-8f8a-acda7a12f66b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "b31fbf66-c7d5-4323-8f8a-acda7a12f66b",
    "outputId": "96ea57c4-1c84-4e8f-e47d-bd8c2bc6a6cc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from typing import Tuple\n",
    "\n",
    "def setup_environment(curr_proj_folder: str = \"pretraining-llms\", google_drive_base_folder: str = \"Colab Notebooks\",\\\n",
    "                      run_remote: bool= True, use_gpu: bool = True) -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Sets up the environment for running code, handling local and remote execution.\n",
    "\n",
    "    Args:\n",
    "        curr_proj_folder (str, optional): Folder name of the current project. Defaults to \"pretraining-llms\".\n",
    "        google_drive_base_folder (str, optional): Folder name of the Google drive base folder. Defaults to \"\"Colab Notebooks\".\n",
    "        use_gpu (bool, optional): Whether to use GPU if available. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,bool]: (computed path_to_scripts,mount_success status)\n",
    "    \"\"\"\n",
    "    # Initialize mount status for Colab\n",
    "    mount_success = False\n",
    "    # Remote run code\n",
    "    if run_remote:\n",
    "      from google.colab import drive\n",
    "      # Mount Google Drive\n",
    "      drive.mount('/content/drive')\n",
    "      # Check if the mount was successful\n",
    "      if os.path.ismount('/content/drive'):\n",
    "        print(\"Google Drive mounted successfully!\")\n",
    "        mount_success = True\n",
    "      else:\n",
    "        print(\"Drive mount failed.\")\n",
    "      # By Default, this is complete mount path\n",
    "      mount_path = '/content/drive/MyDrive'\n",
    "\n",
    "      # complete path to current files\n",
    "      path_to_scripts = os.path.join(mount_path, google_drive_base_folder,curr_proj_folder)\n",
    "      # Create the directory if it doesn't exist\n",
    "      if not os.path.exists(path_to_scripts):\n",
    "        os.makedirs(path_to_scripts)\n",
    "        # change to the path\n",
    "      os.chdir(path_to_scripts)\n",
    "      print(f'Running code in path {os.getcwd()}')\n",
    "    # Local Run\n",
    "    else:\n",
    "      path_to_scripts  = os.getcwd()\n",
    "      # folder name provided as argument should match the one existing\n",
    "      assert os.path.basename(path_to_scripts ) == curr_proj_folder, \\\n",
    "          f\"Folder Name Mismatch: {os.path.basename(path_to_scripts )} != {curr_proj_folder}\"\n",
    "      print(f'Running code in path {path_to_scripts }')\n",
    "    # check GPU usage\n",
    "    if use_gpu:\n",
    "      try:\n",
    "        gpu_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "        print(\"******GPU is available and will be used:**********\")\n",
    "        print(gpu_info)\n",
    "      except subprocess.SubprocessError:\n",
    "        print(\"GPU check failed (nvidia-smi not found or no GPU available). Falling back to CPU.\")\n",
    "        use_gpu = False  # Force CPU usage if GPU check fails\n",
    "    else:\n",
    "        print(\"******use_gpu is set to False. Using CPU******\")\n",
    "    return  path_to_scripts,mount_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb5275-2007-4f4a-8f30-a2ee1c31da43",
   "metadata": {},
   "source": [
    "Always set following parameters as needed before each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd6c74-f107-4b84-9d80-5dd7da23dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project-specific configuration parameters\n",
    "# Specifies the current project folder name\n",
    "curr_proj_folder = \"pretraining-llms\"\n",
    "# Base folder name in Google Drive where notebooks are stored\n",
    "google_drive_base_folder = \"Colab Notebooks\"\n",
    "# Flag to determine whether to use GPU for computations\n",
    "use_gpu = True\n",
    "# Flag to indicate remote execution environment\n",
    "run_remote = False\n",
    "# Flag to control model loading from a specific folder or through URL\n",
    "load_model_from_folder = True\n",
    "\n",
    "if run_remote:\n",
    "  run_local = False\n",
    "  run_local_usingColab = False\n",
    "else:\n",
    "  run_local = False\n",
    "  run_local_usingColab = not run_local\n",
    "\n",
    "# call method to setup environment\n",
    "path_to_scripts,mount_success = setup_environment(curr_proj_folder = curr_proj_folder, \\\n",
    "                                   google_drive_base_folder =  google_drive_base_folder,\\\n",
    "                                    run_remote = run_remote, use_gpu = use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vTKH4bQsbGlP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "id": "vTKH4bQsbGlP",
    "outputId": "e802527f-2612-4d50-fbad-bb6e6a8b16b1"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "image_path = os.path.join(path_to_scripts,\"images\",\"lesson6_benchmarks.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8dd1c-93cf-47be-997d-943f122cb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinySolar-248m-4k\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "print(model_path_or_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b1cfc",
   "metadata": {
    "id": "ac0b1cfc"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Evaluating TinySolar-248m-4k on TruthfulQA MC2\n",
    "\n",
    "### About TruthfulQA MC2\n",
    "TruthfulQA is a benchmark that tests a model's ability to generate truthful responses.\n",
    "- **Multiple-choice format**: The model selects the most truthful answer.\n",
    "- **Why important?**: Many LLMs generate misinformation, and this benchmark helps assess their reliability.\n",
    "- **Source**: [TruthfulQA Paper](https://arxiv.org/abs/2109.07958) and you can checkout the code for implementing the tasks at this [github repo](https://github.com/sylinrl/TruthfulQA).\n",
    "\n",
    "\n",
    "The code below runs only the TruthfulQA MC2 task using the LM Evaluation Harness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2c258-98de-43c7-9b96-cce7a6f20024",
   "metadata": {
    "height": 98,
    "id": "55c2c258-98de-43c7-9b96-cce7a6f20024"
   },
   "outputs": [],
   "source": [
    "model_name = \"TinySolar-248m-4k\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e8c0c1-9a37-4acb-bb6a-a0b50d4b77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Ensure system uses UTF-8 encoding\n",
    "os.environ[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    subprocess.run([\n",
    "        \"lm_eval\", \"--model\", \"hf\",\n",
    "        \"--model_args\", f\"pretrained={model_path_or_name}\",\n",
    "        \"--tasks\", \"truthfulqa_mc2\",\n",
    "        \"--device\", \"auto\",\n",
    "        \"--limit\", \"5\"\n",
    "    ], stdout=f, stderr=subprocess.STDOUT, text=True, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca441ed-c0a3-4e67-84fe-fcc4963448ca",
   "metadata": {},
   "source": [
    "#### Result snapshot is shown below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953609b5-a8d4-4fcc-91b8-d95f18b625ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(path_to_scripts,\"images\",\"lesson6_eval_result.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeef916",
   "metadata": {
    "id": "cbeef916"
   },
   "source": [
    "### Evaluation for the Hugging Face Leaderboard\n",
    "You can use the code below to test your own model against the evaluations required for the [Hugging Face leaderboard](https://huggingface.co/open-llm-leaderboard).\n",
    "\n",
    "\n",
    "\n",
    "The Hugging Face Open LLM Leaderboard ranks models based on standard benchmarks, such as:\n",
    "- **ARC-Challenge**: Tests grade-school science questions.\n",
    "- **HellaSwag**: Evaluates commonsense reasoning.\n",
    "- **MMLU**: Measures model understanding across subjects.\n",
    "- **TruthfulQA**: Assesses truthfulness.\n",
    "- **Winogrande**: Tests pronoun resolution.\n",
    "- **GSM8K**: Evaluates grade-school math.\n",
    "\n",
    "If you decide to run this evaluation on your own model, don't change the few-shot numbers below - they are set by the rules of the leaderboard.\n",
    "\n",
    "### Evaluation Function\n",
    "The function below automates model evaluation across these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc097be-7dbb-4a90-a954-f39d30e8e52c",
   "metadata": {
    "height": 404,
    "id": "7cc097be-7dbb-4a90-a954-f39d30e8e52c"
   },
   "outputs": [],
   "source": [
    "def h6_open_llm_leaderboard(model_name):\n",
    "    \"\"\"\n",
    "    Runs a predefined set of evaluations for a given model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Path to the model to be evaluated.\n",
    "    \"\"\"\n",
    "    task_and_shot = [\n",
    "        ('arc_challenge', 25),\n",
    "        ('hellaswag', 10),\n",
    "        ('mmlu', 5),\n",
    "        ('truthfulqa_mc2', 0),\n",
    "        ('winogrande', 5),\n",
    "        ('gsm8k', 5)\n",
    "    ]\n",
    "\n",
    "    for task, fewshot in task_and_shot:\n",
    "        eval_cmd = f\"\"\"\n",
    "        lm_eval --model hf \\\n",
    "            --model_args pretrained={model_name} \\\n",
    "            --tasks {task} \\\n",
    "            --device cpu \\\n",
    "            --num_fewshot {fewshot}\n",
    "        \"\"\"\n",
    "        os.system(eval_cmd)\n",
    "\n",
    "# Example usage:\n",
    "h6_open_llm_leaderboard(model_name=\"YOUR_MODEL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6da07-0082-4f3f-aa55-9890b146f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_remote  :\n",
    "  from google.colab import drive\n",
    "  drive.flush_and_unmount()\n",
    "  print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
