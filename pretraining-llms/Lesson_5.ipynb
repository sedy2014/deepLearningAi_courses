{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yQZ37zBnOkEG",
   "metadata": {
    "id": "yQZ37zBnOkEG"
   },
   "source": [
    "# Lesson 5: Training Cycle for Large Language Models (LLMs)\n",
    "*  \n",
    "This lesson focuses on understanding and implementing the complete training cycle for LLMs\n",
    "Pretraining is very expensive! Please check costs carefully before starting a pretraining project.\n",
    "*  You can get a rough estimate your training job cost using [this calculator](https://huggingface.co/training-cluster)\n",
    "from Hugging Face. For training on other infrastructure, e.g. AWS or Google Cloud, please consult those providers for up to date cost estimates.\n",
    "\n",
    "## Training Cycle Overview:\n",
    "The training process can be visualized as a four-step cycle:\n",
    "1. **Data Preparation**: Organize and preprocess datasets. This was already done earlier and dataset saved.\n",
    "2. **Hyperparameter Configuration**: Define and tune model parameters for the model created earlier.\n",
    "3. **Training**: Execute the training loop using the `Trainer` class.\n",
    "4. **Monitoring**: Eval and techniques to monitor and debug training performance.\n",
    "\n",
    "\n",
    "    \n",
    "### **1. Data Preparation**\n",
    "\n",
    "The first step in training a language model is preparing your dataset. Hugging Face's `Dataset` class provides tools to handle and preprocess data effectively.\n",
    "\n",
    "- **Key Functions:**\n",
    "  - `len()`: Retrieves the number of samples in the dataset.\n",
    "  - `get_item()`: Accesses individual samples, typically containing `input_ids` (tokenized text) and `labels`.\n",
    "\n",
    "- **Steps:**\n",
    "  - Load your dataset using a library like Hugging Face's `datasets`.\n",
    "  - Tokenize the text using a tokenizer compatible with your model architecture (e.g., GPT or LLaMA).\n",
    "  - Format the dataset to include input features (`input_ids`) and labels for supervised learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Hyperparameter Configuration**\n",
    "\n",
    "Hyperparameters define how your training process operates and significantly influence model performance.\n",
    "- Configure these parameters in the `TrainingArguments` class.\n",
    "- **Key Hyperparameters:**\n",
    "  - **Batch Size:** Number of samples processed together in one forward/backward pass.\n",
    "  - **Learning Rate (LR):** Determines how much to adjust weights during optimization.\n",
    "  - **Warmup Steps:** Gradually increases the learning rate at the start of training to stabilize updates.\n",
    "  - **Weight Decay:** Regularization term to prevent overfitting.\n",
    "  - **Learning Rate Scheduler:** Dynamically adjusts the learning rate during training.\n",
    "\n",
    "- **Configuration Considerations:**\n",
    "  - Adjust hyperparameters based on hardware limitations and model size.\n",
    "  - Use pre-defined configurations from academic literature or previous experiments as a starting point.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Model Training**\n",
    "\n",
    "The Hugging Face `Trainer` class simplifies the training process by integrating data, models, and hyperparameters.\n",
    "\n",
    "- **Inputs:**\n",
    "  - **Data:** Tokenized datasets for training and evaluation.\n",
    "  - **Model:** Pre-trained or randomly initialized transformer model.\n",
    "  - **Config:** Hyperparameter configurations defined earlier.\n",
    "\n",
    "- **Steps:**\n",
    "  - Initialize your model using pre-trained weights or random initialization.\n",
    "  - Define a `Trainer` instance with your model, datasets, and training arguments.\n",
    "  - Begin training while monitoring metrics like loss and validation accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Monitoring**\n",
    "\n",
    "Monitoring is essential to ensure that your training process is progressing as expected and to identify potential issues early.\n",
    "\n",
    "- **Metrics to Monitor:**\n",
    "  - **Loss:** Indicates how well the model is learning.\n",
    "  - **Validation Accuracy:** Measures generalization capability.\n",
    "  - **Resource Usage:** Tracks GPU/CPU utilization and memory consumption.\n",
    "\n",
    "- **Tools for Monitoring:**\n",
    "  - Built-in logging features in frameworks like Hugging Face's `Trainer`.\n",
    "  - External tools like TensorBoard or Weights & Biases for advanced visualization and analysis.\n",
    "\n",
    "---\n",
    "**Next Steps**:\n",
    "- Experiment with different hyperparameter configurations to improve model performance.\n",
    "- Explore advanced optimization techniques such as mixed precsion (`fp16`) and gradient checkpointing for memory efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lHOSajGNO7i1",
   "metadata": {
    "id": "lHOSajGNO7i1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from typing import Tuple\n",
    "\n",
    "def setup_environment(curr_proj_folder: str = \"pretraining-llms\", google_drive_base_folder: str = \"Colab Notebooks\",\\\n",
    "                      run_remote: bool= True, use_gpu: bool = True) -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Sets up the environment for running code, handling local and remote execution.\n",
    "\n",
    "    Args:\n",
    "        curr_proj_folder (str, optional): Folder name of the current project. Defaults to \"pretraining-llms\".\n",
    "        google_drive_base_folder (str, optional): Folder name of the Google drive base folder. Defaults to \"\"Colab Notebooks\".\n",
    "        use_gpu (bool, optional): Whether to use GPU if available. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,bool]: (computed path_to_scripts,mount_success status)\n",
    "    \"\"\"\n",
    "    # Initialize mount status for Colab\n",
    "    mount_success = False\n",
    "    # Remote run code\n",
    "    if run_remote:\n",
    "      from google.colab import drive\n",
    "      # Mount Google Drive\n",
    "      drive.mount('/content/drive')\n",
    "      # Check if the mount was successful\n",
    "      if os.path.ismount('/content/drive'):\n",
    "        print(\"Google Drive mounted successfully!\")\n",
    "        mount_success = True\n",
    "      else:\n",
    "        print(\"Drive mount failed.\")\n",
    "      # By Default, this is complete mount path\n",
    "      mount_path = '/content/drive/MyDrive'\n",
    "\n",
    "      # complete path to current files\n",
    "      path_to_scripts = os.path.join(mount_path, google_drive_base_folder,curr_proj_folder)\n",
    "      # Create the directory if it doesn't exist\n",
    "      if not os.path.exists(path_to_scripts):\n",
    "        os.makedirs(path_to_scripts)\n",
    "        # change to the path\n",
    "      os.chdir(path_to_scripts)\n",
    "      print(f'Running code in path {os.getcwd()}')\n",
    "    # Local Run\n",
    "    else:\n",
    "      path_to_scripts  = os.getcwd()\n",
    "      # folder name provided as argument should match the one existing\n",
    "      assert os.path.basename(path_to_scripts ) == curr_proj_folder, \\\n",
    "          f\"Folder Name Mismatch: {os.path.basename(path_to_scripts )} != {curr_proj_folder}\"\n",
    "      print(f'Running code in path {path_to_scripts }')\n",
    "    # check GPU usage\n",
    "    if use_gpu:\n",
    "      try:\n",
    "        gpu_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "        print(\"******GPU is available and will be used:**********\")\n",
    "        print(gpu_info)\n",
    "      except subprocess.SubprocessError:\n",
    "        print(\"GPU check failed (nvidia-smi not found or no GPU available). Falling back to CPU.\")\n",
    "        use_gpu = False  # Force CPU usage if GPU check fails\n",
    "    else:\n",
    "        print(\"******use_gpu is set to False. Using CPU******\")\n",
    "    return  path_to_scripts,mount_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db999331-4914-4dac-8525-d03e8d9ae330",
   "metadata": {
    "id": "db999331-4914-4dac-8525-d03e8d9ae330"
   },
   "source": [
    "Always set following parameters as needed before each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b16064-df88-4ac4-ac3d-ec7deb240caf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89b16064-df88-4ac4-ac3d-ec7deb240caf",
    "outputId": "2c96f20f-918c-40c4-859c-e127dddaeca5"
   },
   "outputs": [],
   "source": [
    "# Project-specific configuration parameters\n",
    "# Specifies the current project folder name\n",
    "curr_proj_folder = \"pretraining-llms\"\n",
    "# Base folder name in Google Drive where notebooks are stored\n",
    "google_drive_base_folder = \"Colab Notebooks\"\n",
    "# Flag to determine whether to use GPU for computations\n",
    "use_gpu = True\n",
    "# Flag to indicate remote execution environment\n",
    "run_remote = True\n",
    "# Flag to control model loading from a specific folder or through URL\n",
    "# NOTE: we will be using pretrained model saved locally\n",
    "load_model_from_folder = True\n",
    "\n",
    "if run_remote:\n",
    "  run_local = False\n",
    "  run_local_usingColab = False\n",
    "else:\n",
    "  run_local = False\n",
    "  run_local_usingColab = not run_local\n",
    "\n",
    "# call method to setup environment\n",
    "path_to_scripts,mount_success = setup_environment(curr_proj_folder = curr_proj_folder, \\\n",
    "                                   google_drive_base_folder =  google_drive_base_folder,\\\n",
    "                                    run_remote = run_remote, use_gpu = use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-7X-N76xPCAC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "-7X-N76xPCAC",
    "outputId": "aa5208ad-3bbc-40b4-8b7e-5e55f5c0d621"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "image_path = os.path.join(path_to_scripts,\"images\",\"lesson5_train_cycle.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe0bb8-d33b-4064-aedf-ad008d5fbf2c",
   "metadata": {
    "height": 47,
    "id": "31fe0bb8-d33b-4064-aedf-ad008d5fbf2c"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# check torch versions installed\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "if use_gpu:\n",
    "    print(\"CUDA runtime version:\", torch.version.cuda)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3010e",
   "metadata": {
    "id": "fda3010e"
   },
   "source": [
    "### 1. Load the model to be trained\n",
    "\n",
    "Load the upscaled model we saved in **//saved_model//TinySolar-308m-4k-init** from the previous lesson:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a966b4d-3b6f-40a0-a912-d6f0d247f088",
   "metadata": {
    "height": 166,
    "id": "6a966b4d-3b6f-40a0-a912-d6f0d247f088"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_name = \"TinySolar-308m-4k-init\"\n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"savedModel\",model_name)\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e3729-5101-418b-9d0b-55ffeec9d999",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "970e3729-5101-418b-9d0b-55ffeec9d999",
    "outputId": "a1696d6b-a294-490d-ed21-c3b30782e86b"
   },
   "outputs": [],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d08680-8993-4904-9bab-34b27d8cba57",
   "metadata": {
    "id": "32d08680-8993-4904-9bab-34b27d8cba57"
   },
   "source": [
    "### 2. Load dataset\n",
    "\n",
    "* Here  we will  update two methods on the `Dataset` object to allow it to interface with the trainer\n",
    "\n",
    "* These will be applied when we  specify the dataset that was  created in Lesson 3 as the training data and  stored in **.//saved_pretrain_cleaned_data//packaged_pretrained_dataset.json**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade55c1-3296-4c83-97a0-2579471829ab",
   "metadata": {
    "height": 489,
    "id": "3ade55c1-3296-4c83-97a0-2579471829ab"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "import datasets\n",
    "from torch.utils.data import Dataset\n",
    "dataset_path = \".//saved_pretrain_cleaned_data//packaged_pretrained_dataset.json\"\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, split=\"train\"):\n",
    "        \"\"\"Initializes the custom dataset object.\"\"\"\n",
    "        self.args = args\n",
    "        # Loads the dataset from a Json  file\n",
    "        self.dataset = datasets.load_dataset(\n",
    "                     \"json\",  data_files= args.dataset_name,\n",
    "                     split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        # PyTorch uses this method to determine how many batches are needed during training.\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns input and op token for each example\n",
    "        at the specified index in dictionary format :allows PyTorch to access individual data points\n",
    "         using an index \"\"\"\n",
    "        # Convert the lists to a LongTensor for PyTorch\n",
    "        # In next-word prediction (language modeling), the labels are the same as the input IDs.\n",
    "        # \"input_ids: The processed tokenized input sequence.\n",
    "        # \"labels\": The same input sequence, used as target output for training.\n",
    "        input_ids = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "        labels = torch.LongTensor(self.dataset[idx][\"input_ids\"])\n",
    "        # Return the sample as a dictionary\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504499ec-ff8a-4377-b5f1-f2061a09e681",
   "metadata": {
    "id": "504499ec-ff8a-4377-b5f1-f2061a09e681"
   },
   "source": [
    "### 3. Configure Training Arguments\n",
    "\n",
    "Here , We  set up the training run. The training dataset you created in Lesson 3 is specified in the Dataset configuration section.\n",
    "Some of common parameters to change/work with are:\n",
    "* **optim** : Training optimizers: LLM's commonly used adamw_torch\n",
    "* **max_steps**: Number of maximum training steps\n",
    " With Pre-training, Its common practice to set the num_train_epochs =1 , instead of setting max_steps, to process all data at once\n",
    "* **per_device_train_batch_size** : **rule of thumb is to use largest batch size allowed per memory** e.g,\n",
    "  the created dataset has Max seq length as 32 tokens, so batch size is 64 tokens (for  batch size as 2).\n",
    "   if there are 8 GPU available, it will be 8*64-512 tokens per training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab5882-4547-43ad-a271-4f5db42d8075",
   "metadata": {
    "height": 591,
    "id": "0eab5882-4547-43ad-a271-4f5db42d8075"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import transformers\n",
    "\n",
    "@dataclass\n",
    "class CustomArguments(transformers.TrainingArguments):\n",
    "    dataset_name: str = field(                           # Dataset configuration\n",
    "        default=\"./packaged_pretrained_dataset.json\")\n",
    "    num_proc: int = field(default=1)                     # Number of subprocesses for data preprocessing\n",
    "    max_seq_length: int = field(default=32)              # Maximum sequence length\n",
    "\n",
    "    # Core training configurations\n",
    "    seed: int = field(default=0)                         # Random seed for initialization, ensuring reproducibility\n",
    "    optim: str = field(default=\"adamw_torch\")            # Optimizer, here it's AdamW implemented in PyTorch\n",
    "    max_steps: int = field(default=30)                   # Number of maximum training steps\n",
    "    per_device_train_batch_size: int = field(default=2)  # Batch size per device during training\n",
    "\n",
    "    # Other training configurations\n",
    "    learning_rate: float = field(default=5e-5)           # Initial learning rate for the optimizer\n",
    "    weight_decay: float = field(default=0)               # Weight decay\n",
    "    warmup_steps: int = field(default=10)                # Number of steps for the learning rate warmup phase\n",
    "    lr_scheduler_type: str = field(default=\"linear\")     # Type of learning rate scheduler\n",
    "    gradient_checkpointing: bool = field(default=True)   # Enable gradient checkpointing to save memory\n",
    "    dataloader_num_workers: int = field(default=2)       # Number of subprocesses for data loading\n",
    "    bf16: bool = field(default=True)                     # Use bfloat16 precision for training on supported hardware\n",
    "    gradient_accumulation_steps: int = field(default=1)  # Number of steps to accumulate gradients before updating model weights\n",
    "\n",
    "    # Logging configuration\n",
    "    logging_steps: int = field(default=3)                # Frequency of logging training information\n",
    "    report_to: str = field(default=\"none\")               # Destination for logging (e.g., WandB, TensorBoard)\n",
    "\n",
    "    # Saving configuration : for intermediate checpoints\n",
    "    save_strategy: str = field(default=\"steps\")          # Can be replaced with \"epoch\"\n",
    "    save_steps: int = field(default=3)                   # Frequency of saving training checkpoint\n",
    "    save_total_limit: int = field(default=2)             # The total number of checkpoints to be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d62ed",
   "metadata": {
    "id": "4a4d62ed"
   },
   "source": [
    "# Create Hugging Face Argument Parser\n",
    "\n",
    "This is used to parse the input arguments and also add argument to set the output directory where the model will be saved: The `HfArgumentParser` class automatically generates the necessary `argparse` arguments based on the dataclass structure.\n",
    "\n",
    "`args, = parser.parse_args_into_dataclasses(args=[\"--output_dir\", \"output\"])` does the following:  \n",
    "Instead of reading arguments from the command line (`sys.argv`), it uses the explicitly provided list `[\"--output_dir\", \"output\"]`.  \n",
    "The reason `--output_dir` is needed is that the `CustomArguments` class inherits from `transformers.TrainingArguments`, which requires an `output_dir` parameter.  \n",
    "\n",
    "It parses these arguments and converts them into an instance of your `CustomArguments` dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af55fd9-de91-4f01-8a5b-f16fb6d7b921",
   "metadata": {
    "height": 81,
    "id": "8af55fd9-de91-4f01-8a5b-f16fb6d7b921"
   },
   "outputs": [],
   "source": [
    "parser = transformers.HfArgumentParser(CustomArguments)\n",
    "# pass the dataset path to the CustomArguments  object overriding default one\n",
    "args, = parser.parse_args_into_dataclasses(\n",
    "    args=[\"--output_dir\", \"output\",\n",
    "          \"--dataset_name\", dataset_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24167fe5-6e3d-400f-a1e3-f1ed42d062ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24167fe5-6e3d-400f-a1e3-f1ed42d062ac",
    "outputId": "fb998ac3-a093-4acf-f31a-64ebd0f1ad33"
   },
   "outputs": [],
   "source": [
    "print(f\"Dataset path: {args.dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184f35a",
   "metadata": {
    "id": "1184f35a"
   },
   "source": [
    "As we pass arguments to the custom dataset, it will be configured as needed for the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812abcd8-1146-44fc-8e3e-65a5999eea0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "60a34f1981f04a148014f69186c1e393",
      "350f0506b3244644addcc8984090aa8f",
      "99877d55df5d4a9fb49005277bedbe30",
      "bc92fea0db8146319a248bcd3345afc2",
      "ebac22d0ad07450e8f769dd7979499b8",
      "dc72371ab6094862b81f8a689969fb9d",
      "43aa5e2ec2c3472693f9445996994170",
      "54211e10cdba4157ac91ed1594af23d9",
      "532fb2f5d7e24e41940178c19e0bc9e3",
      "8c0782e7b1c64e828731c0e9f1759f0d",
      "e2efa81cdaff4e7eaa78a0392737ad6e"
     ]
    },
    "height": 30,
    "id": "812abcd8-1146-44fc-8e3e-65a5999eea0c",
    "outputId": "06af8cff-0d61-4deb-9ce3-c9035ec0b4a9"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0fb13a",
   "metadata": {
    "id": "9a0fb13a"
   },
   "source": [
    "Check the shape of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928c414-298d-4524-b078-ad1e844407a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "0928c414-298d-4524-b078-ad1e844407a7",
    "outputId": "2d3ef128-2be1-4a56-c3d6-b587dcbf93a3"
   },
   "outputs": [],
   "source": [
    "print(\"Input shape: \", train_dataset[0]['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3b247-d377-463b-82eb-ec0514c32949",
   "metadata": {
    "id": "74a3b247-d377-463b-82eb-ec0514c32949"
   },
   "source": [
    "## 4. Run the trainer and monitor the loss\n",
    "\n",
    "First, set up a callback to log the loss values during training (note this cell is not shown in the video):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc3a27-9d9a-4caf-bfc5-01f8eddcacf7",
   "metadata": {
    "height": 234,
    "id": "91dc3a27-9d9a-4caf-bfc5-01f8eddcacf7"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "# Define a custom callback to log the loss values\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            self.logs.append(logs)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "# Initialize the callback\n",
    "loss_logging_callback = LossLoggingCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f01b6",
   "metadata": {
    "id": "816f01b6"
   },
   "source": [
    "Then, create an instance of the Hugging Face `Trainer` object from the `transformers` library. Call the `train()` method of the trainder to initialize the training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9-GBU86MBarm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "9-GBU86MBarm",
    "outputId": "ff9acf31-4b94-435c-9859-cc0328860e95"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Update training arguments for faster training\n",
    "args.num_train_epochs = 5  # Set the number of epochs to 2\n",
    "args.max_steps = 10 # Set a maximum number of steps\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pretrained_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    callbacks=[loss_logging_callback]\n",
    ")\n",
    "\n",
    "# Instead of manual loop, call the train method directly:\n",
    "trainer.train() # This will handle the training loop internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee517564-e1d4-4e15-8ca9-6d0c0e7ea7e6",
   "metadata": {
    "height": 200,
    "id": "ee517564-e1d4-4e15-8ca9-6d0c0e7ea7e6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2f6dac",
   "metadata": {
    "id": "ac2f6dac"
   },
   "source": [
    "You can use the code below to save intermediate model checkpoints in your own training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c395d1",
   "metadata": {
    "height": 81,
    "id": "b4c395d1"
   },
   "outputs": [],
   "source": [
    "# Saving configuration\n",
    "    # save_strategy: str = field(default=\"steps\")          # Can be replaced with \"epoch\"\n",
    "    # save_steps: int = field(default=3)                   # Frequency of saving training checkpoint\n",
    "    # save_total_limit: int = field(default=2)             # The total number of checkpoints to be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d4e75",
   "metadata": {
    "id": "ee5d4e75"
   },
   "source": [
    "### Checking the performance of an intermediate checkpoint\n",
    "\n",
    "Below, you can try generating text using an intermediate checkpoint of the model. This checkpoint was saved after 10,000 training steps.  \n",
    "As in  previous lessons, we use  the Solar tokenizer and then set up a `TextStreamer` object to display the text as it is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f45984-36b0-4351-a758-b67f674d9e2e",
   "metadata": {
    "height": 64,
    "id": "29f45984-36b0-4351-a758-b67f674d9e2e"
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "model_name = \"TinySolar-248m-4k\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/')\n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589b8e5-839f-4c98-8937-ddb60c324d85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 183,
    "id": "f589b8e5-839f-4c98-8937-ddb60c324d85",
    "outputId": "ae04695c-054d-42db-df51-413d551c976f"
   },
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "from transformers import AutoTokenizer, TextStreamer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "checkpoint_name = \"checkpoint-10\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",\"output\",model_name).replace('\\\\', '/')\n",
    "# download Checkpoint if not present  locally\n",
    "checkPoint_path_or_name = os.path.join(path_to_scripts ,\"output\",checkpoint_name) if load_model_from_folder else upstage_path\n",
    "print(f'using checkpont from path {checkPoint_path_or_name}')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkPoint_path_or_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86beb9b4-fb8f-4ac1-ab25-f95fe11e7d8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 319,
    "id": "86beb9b4-fb8f-4ac1-ab25-f95fe11e7d8c",
    "outputId": "9e537bd4-cf5f-4419-e356-3396fbc04d97"
   },
   "outputs": [],
   "source": [
    "# run inference\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YoZHZZF4gCxJ",
   "metadata": {
    "id": "YoZHZZF4gCxJ"
   },
   "source": [
    "### Result\n",
    "`when people ask me anything like that, so, for me, my passion to become the next leader and not just of mine was really important.\n",
    "One question I have for...'`\n",
    "\n",
    "Notes:\n",
    "* **Its better  than before** that its **not repeating again and again**\n",
    "*  dosnt make ton of sence as this small model cannot generate long stretch of senetence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda6595-7b27-4cea-a3e5-83f80f7ab2b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeda6595-7b27-4cea-a3e5-83f80f7ab2b9",
    "outputId": "8ad5684f-34a9-4949-cb5a-11a6a184c52d"
   },
   "outputs": [],
   "source": [
    "if run_remote  :\n",
    "  from google.colab import drive\n",
    "  drive.flush_and_unmount()\n",
    "  print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "350f0506b3244644addcc8984090aa8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc72371ab6094862b81f8a689969fb9d",
      "placeholder": "​",
      "style": "IPY_MODEL_43aa5e2ec2c3472693f9445996994170",
      "value": "Generating train split: "
     }
    },
    "43aa5e2ec2c3472693f9445996994170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "532fb2f5d7e24e41940178c19e0bc9e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54211e10cdba4157ac91ed1594af23d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "60a34f1981f04a148014f69186c1e393": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_350f0506b3244644addcc8984090aa8f",
       "IPY_MODEL_99877d55df5d4a9fb49005277bedbe30",
       "IPY_MODEL_bc92fea0db8146319a248bcd3345afc2"
      ],
      "layout": "IPY_MODEL_ebac22d0ad07450e8f769dd7979499b8"
     }
    },
    "8c0782e7b1c64e828731c0e9f1759f0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99877d55df5d4a9fb49005277bedbe30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54211e10cdba4157ac91ed1594af23d9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_532fb2f5d7e24e41940178c19e0bc9e3",
      "value": 1
     }
    },
    "bc92fea0db8146319a248bcd3345afc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c0782e7b1c64e828731c0e9f1759f0d",
      "placeholder": "​",
      "style": "IPY_MODEL_e2efa81cdaff4e7eaa78a0392737ad6e",
      "value": " 15510/0 [00:00&lt;00:00, 20410.65 examples/s]"
     }
    },
    "dc72371ab6094862b81f8a689969fb9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2efa81cdaff4e7eaa78a0392737ad6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebac22d0ad07450e8f769dd7979499b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
