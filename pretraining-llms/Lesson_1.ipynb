{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "phiae0Iq0qpD",
   "metadata": {
    "id": "phiae0Iq0qpD"
   },
   "source": [
    "# Lesson 1: Pre-Training and Model Performance\r\n",
    "### Compare how text generation varies between\n",
    "   **base general model**, a **fine-tuned model**,and a **specialized pre-trained model**.s:\r\n",
    "\r\n",
    "## Key Objectives:\r\n",
    "1. Compare text generation between:\r\n",
    "   - Base general model\r\n",
    "   - Fine-tuned model\r\n",
    "   - Specialized pre-trained model\r\n",
    "\r\n",
    "## Core Components:\r\n",
    "1. **Environment Setup**: Configures local or remote (Colab) runtime with GPU support\r\n",
    "2. **Model Loading**: Initializes different model types for comparison\r\n",
    "3. **Text Generation**: Implements and compares generation across model variants\r\n",
    "4. **Performance Analysis**: Evaluates and contrasts output quality and task-specific performance\r\n",
    "\r\n",
    "## Key Concepts Covered:\r\n",
    "- Transfer learning in NLP\r\n",
    "- Impact of domain-specific pre-training\r\n",
    "- Fine-tuning vs. specialized pre-training trade-offs\r\n",
    "\r\n",
    "## Practical Outcomes:\r\n",
    "- Understanding the effects of pre-training on model performance\r\n",
    "- Insights into choosing appropriate model types for specific tasks\r\n",
    "- Hands-on experience with different LLM variants\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70d8f0-cbc0-4252-bc54-0090a35c1cf8",
   "metadata": {
    "id": "dd70d8f0-cbc0-4252-bc54-0090a35c1cf8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from typing import Tuple\n",
    "\n",
    "def setup_environment(curr_proj_folder: str = \"pretraining-llms\", google_drive_base_folder: str = \"Colab Notebooks\",\\\n",
    "                      run_remote: bool= True, use_gpu: bool = True) -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Sets up the environment for running code, handling local and remote execution.\n",
    "\n",
    "    Args:\n",
    "        curr_proj_folder (str, optional): Folder name of the current project. Defaults to \"pretraining-llms\".\n",
    "        google_drive_base_folder (str, optional): Folder name of the Google drive base folder. Defaults to \"\"Colab Notebooks\".\n",
    "        use_gpu (bool, optional): Whether to use GPU if available. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,bool]: (computed path_to_scripts,mount_success status)\n",
    "    \"\"\"\n",
    "    # Initialize mount status for Colab\n",
    "    mount_success = False\n",
    "    # Remote run code\n",
    "    if run_remote:\n",
    "      from google.colab import drive\n",
    "      # Mount Google Drive\n",
    "      drive.mount('/content/drive')\n",
    "      # Check if the mount was successful\n",
    "      if os.path.ismount('/content/drive'):\n",
    "        print(\"Google Drive mounted successfully!\")\n",
    "        mount_success = True\n",
    "      else:\n",
    "        print(\"Drive mount failed.\")\n",
    "      # By Default, this is complete mount path\n",
    "      mount_path = '/content/drive/MyDrive'\n",
    "\n",
    "      # complete path to current files\n",
    "      path_to_scripts = os.path.join(mount_path, google_drive_base_folder,curr_proj_folder)\n",
    "      # Create the directory if it doesn't exist\n",
    "      if not os.path.exists(path_to_scripts):\n",
    "        os.makedirs(path_to_scripts)\n",
    "        # change to the path\n",
    "      os.chdir(path_to_scripts)\n",
    "      print(f'Running code in path {os.getcwd()}')\n",
    "    # Local Run\n",
    "    else:\n",
    "      path_to_scripts  = os.getcwd()\n",
    "      # folder name provided as argument should match the one existing\n",
    "      assert os.path.basename(path_to_scripts ) == curr_proj_folder, \\\n",
    "          f\"Folder Name Mismatch: {os.path.basename(path_to_scripts )} != {curr_proj_folder}\"\n",
    "      print(f'Running code in path {path_to_scripts }')\n",
    "    # check GPU usage\n",
    "    if use_gpu:\n",
    "      try:\n",
    "        gpu_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "        print(\"******GPU is available and will be used:**********\")\n",
    "        print(gpu_info)\n",
    "      except subprocess.SubprocessError:\n",
    "        print(\"GPU check failed (nvidia-smi not found or no GPU available). Falling back to CPU.\")\n",
    "        use_gpu = False  # Force CPU usage if GPU check fails\n",
    "    else:\n",
    "        print(\"******use_gpu is set to False. Using CPU******\")\n",
    "    return  path_to_scripts,mount_success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QGPlnv9-PqQ1",
   "metadata": {
    "id": "QGPlnv9-PqQ1"
   },
   "source": [
    "### Always set  following parameters before each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_6F11bcXPdSd",
   "metadata": {
    "id": "_6F11bcXPdSd"
   },
   "outputs": [],
   "source": [
    "# Project-specific configuration parameters\n",
    "# Specifies the current project folder name\n",
    "curr_proj_folder = \"pretraining-llms\"\n",
    "# Base folder name in Google Drive where notebooks are stored\n",
    "google_drive_base_folder = \"Colab Notebooks\"\n",
    "# Flag to determine whether to use GPU for computations\n",
    "use_gpu = True\n",
    "# Flag to indicate remote execution environment\n",
    "run_remote = False\n",
    "# Flag to control model loading from a specific folder or through URL\n",
    "load_model_from_folder = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sjcy0EVoPjci",
   "metadata": {
    "id": "Sjcy0EVoPjci"
   },
   "source": [
    "## Possible Scenarios\n",
    "1. Remote Mode Active:\n",
    "   - `run_remote = True`\n",
    "   - `run_local = False`\n",
    "   - `run_local_usingColab = False`\n",
    "**Scenario: Running on Google Colab**\n",
    "2. Remote Mode Disabled:\n",
    "   - `run_remote = False`\n",
    "   - `run_local = True`\n",
    "   - `run_local_usingColab = False`\n",
    "**Scenario: Running on Local Computer with Jupyter Lab**\n",
    "2. Remote Mode Disabled:\n",
    "   - `run_remote = False`\n",
    "   - `run_local = False`\n",
    "   - `run_local_usingColab = True`\n",
    "**Scenario: Running on Google Colab using Local PC compute resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9W2Ykps4ybG3",
   "metadata": {
    "id": "9W2Ykps4ybG3"
   },
   "outputs": [],
   "source": [
    "if run_remote:\n",
    "  run_local = False\n",
    "  run_local_usingColab = False\n",
    "else:\n",
    "  run_local = False\n",
    "  run_local_usingColab = not run_local\n",
    "\n",
    "# call method to setup environment\n",
    "path_to_scripts,mount_success = setup_environment(curr_proj_folder = curr_proj_folder, \\\n",
    "                                   google_drive_base_folder =  google_drive_base_folder,\\\n",
    "                                    run_remote = run_remote, use_gpu = use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yEX7e7C1jYNi",
   "metadata": {
    "id": "yEX7e7C1jYNi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_path = os.path.join(path_to_scripts,\"images\",\"pretrain_diag.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af366c49-944d-4ad3-9bf9-cf0b5e386cc6",
   "metadata": {
    "id": "af366c49-944d-4ad3-9bf9-cf0b5e386cc6"
   },
   "source": [
    "# Lesson Summary: Pre-Training and Model Performance\n",
    "\n",
    "## What is Pre-Training?\n",
    "- Is a process of taking a model, **generally a transformer neural network,** and training it **from  scratch  on a large corpus of text** using supervised learning so that it learns to **repeatedly predict the next token given an input prompt.**  \n",
    "This is shown above , where **Each text sample is turned into many\n",
    "input-output pairs**, like you see here. Over time, the model learns to correctly\n",
    "predict the next word.\n",
    "\n",
    "- Called pre-training because it is the **first step of training\n",
    "an LLM before any fine-tuning**  to have it follow instructions or further alignment to human preferences is carried out.\n",
    "\n",
    "**Output of Pre-Training**\n",
    "- Produces a **base model**, which could be\n",
    "1.  **Trained from scratch** (Can be costly)\n",
    "2.  **ReTrained** or a **Fine Tuned** on specific task\n",
    "\n",
    "\n",
    "**Where Pre-Training excels:**\n",
    "*  Build tasks in specific domains\n",
    "*  Stronger ability in  other languages\n",
    "\n",
    "**Depth Upscaling**\n",
    "* Creates new LLM by duplicating layers of smaller pre-trained model.\n",
    "* this is further pre-trained -> larger batter model\n",
    "*  can be 70% less costly than traditional pre-training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U5d-UOA6TwmE",
   "metadata": {
    "id": "U5d-UOA6TwmE"
   },
   "source": [
    "# Detailed explanation of Pre-training methods\n",
    " This is taken from the paper\n",
    "\n",
    " [Continual Learning for Large Language Models: A Survey\"](https://arxiv.org/abs/2402.01364)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6lGHO52VX11C",
   "metadata": {
    "id": "6lGHO52VX11C"
   },
   "outputs": [],
   "source": [
    "image_path = os.path.join(path_to_scripts,\"images\",\"lesson1_llmpaper1.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)\n",
    "image_path = os.path.join(path_to_scripts,\"images\",\"lesson1_llmpaper2.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c33ce9-8439-4626-9cf6-4ee7ad5a9e4f",
   "metadata": {
    "id": "91c33ce9-8439-4626-9cf6-4ee7ad5a9e4f"
   },
   "source": [
    "### Start Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cb8a4-1893-4774-8ea3-4dfb770d1544",
   "metadata": {
    "id": "lTI44nsVgcAx"
   },
   "outputs": [],
   "source": [
    "# check torch versions installed\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "if use_gpu:\n",
    "    print(\"CUDA runtime version:\", torch.version.cuda)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03d167-6ebc-4da9-87c8-11ce1bd4eeaa",
   "metadata": {
    "id": "bb03d167-6ebc-4da9-87c8-11ce1bd4eeaa"
   },
   "outputs": [],
   "source": [
    "# Suppress warning messages for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iU7OuWGrzKin",
   "metadata": {
    "id": "iU7OuWGrzKin"
   },
   "source": [
    "**Setting Random Seed**  \n",
    "Setting a random seed ensures reproducibility of results across different runs.\n",
    "The number 42 is commonly used as a default seed (reference to \"The Hitchhiker's Guide to the Galaxy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57a778-0f12-4e07-93b1-c1f41c09c06d",
   "metadata": {
    "id": "8b23a1a8-096f-4563-ab87-bbf3c2e699a2"
   },
   "outputs": [],
   "source": [
    "def fix_torch_seed(seed=42):\n",
    "    \"\"\"\n",
    "    This function sets various random seeds in PyTorch to ensure reproducible results\n",
    "    The default seed is 42, but can be changed by passing a different value\n",
    "    \"\"\"\n",
    "\n",
    "    # Sets the seed for generating random numbers for CPU operations\n",
    "    # This affects all random number generation in PyTorch for CPU computations\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Sets the seed for generating random numbers on CUDA (GPU) operations\n",
    "    # This ensures reproducibility when using GPU acceleration\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # When True, ensures that CUDA selects deterministic algorithms\n",
    "    # This may slow down performance but guarantees reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # When False, prevents CUDA from auto-tuning algorithms\n",
    "    # This disables the automatic selection of the best algorithm\n",
    "    # which could change between runs and affect reproducibility\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call the function with default seed value (42)\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b9da4-fe8b-44fa-bbf9-cc910fbac36a",
   "metadata": {},
   "source": [
    "## 2.1 Load a general pretrained model\n",
    "\n",
    "This course will work with small models that fit within the memory of the learning platform.\n",
    "* **TinySolar-248m-4k is a small decoder-only model** with **248M parameters** (similar in scale to GPT2) and a **4096 token context window**. \n",
    "   You can find the model on the Hugging Face model library at [this link](https://huggingface.co/upstage/TinySolar-248m-4k).\n",
    "Load the model in three steps:\n",
    "1. Specify the path to the model in the Hugging Face model library\n",
    "2. Load the model using `AutoModelforCausalLM` in the `transformers` library\n",
    "3. Load the tokenizer for the model from the same model path  \n",
    "\n",
    "### Notes:  \n",
    "### AutoModelForCausalLM :  \n",
    "is a specialized class in the Hugging Face Transformers library designed to facilitate causal (autoregressive) language modeling.\n",
    "\n",
    "* The class **automatically selects the appropriate model architecture** (e.g., GPT, GPT-2, GPT-Neo) based on the provided **model identifier.**\n",
    "\n",
    "* Unlike **AutoModel**, which outputs  only the hidden state embedded representations,   \n",
    "    **AutoModelForCausalLM** appends an additional linear layer (a language modeling head) on top of the base model.   \n",
    "    This **head maps the dense hidden states to a sparse representation corresponding to the probabilities for each token in the vocabulary**.   \n",
    "  This extra component is essential for generating meaningful text since it converts the output into real-word predictions.  \n",
    "   for example, if you wanted to do a classification task with BERT, using AutoModelForSequenceClassification would load the BERT model  \n",
    "   and an additional layer that maps the BERT embeddings to one of your classification labels.  \n",
    "   If you just use AutoModel to load the BERT model, your output would just be raw BERT embeddings.\n",
    "\n",
    "* The class is optimized for tasks such as **auto-completion, creative writing, dialogue generation**\n",
    "  and other applications where you need to *predict subsequent tokens* from a given prompt.\n",
    "### AutoTokenizer     \n",
    "is a high-level class in the Hugging Face Transformers library that automatically **selects and loads the appropriate tokenizer for a given pretrained model**\n",
    "\n",
    "**Key Features and Functionality**  \n",
    "1. **Automatic Selection of the Right Tokenizer**  \n",
    "When initilaized with a model name or path (using the **from_pretrained method**), it inspects the model configuration and automatically  \n",
    "    instantiates the correct subclass (for example, a BERT-based tokenizer, a GPT-2 tokenizer, etc.)\n",
    "\n",
    "2. **Uniform Interface for Different Tokenizers**\n",
    "Regardless of whether the underlying tokenizer is a **traditional Python implementation or the faster Rust-based “Fast” version**,   \n",
    "AutoTokenizer **exposes a consistent and easy-to-use interface**. You can call methods like\\__call__, encode, and decode without worrying about the underlying implementation details.\n",
    "\n",
    "3. **Handling Special Tokens and Configuration**:\n",
    "Aautomatically handles special tokens (such as [CLS], [SEP], or padding tokens) that a model needs during training   \n",
    "and inference: ensuring that the  inputs are correctly formatted for the model.\n",
    "\n",
    "4. **Efficient Tokenization**\n",
    "By default, if a **_Fast tokenizer_** is available, AutoTokenizer instantiates that version.   \n",
    "Fast tokenizers are **built on top of the Hugging Face Tokenizers library**,   \n",
    "which is highly optimized (in Rust) for batch tokenization and provides additional utilities such as **_mapping between tokens and characters_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405c72c-21d5-4a12-bbad-08a6e25be383",
   "metadata": {
    "id": "8405c72c-21d5-4a12-bbad-08a6e25be383"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"TinySolar-248m-4k\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "# choose device map as CPU or Auto for GPU support\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load this model: generic and small\n",
    "tiny_general_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    # Specify precision for efficiency\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the tokenizer by specifying just model path/name\n",
    "tiny_general_tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90984c4-acbd-4abb-b6eb-05869535bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"TinySolar-248m-4k\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "# choose device map as CPU or Auto for GPU support\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load this model: generic and small\n",
    "tiny_general_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    # Specify precision for efficiency\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the tokenizer by specifing just model path/name\n",
    "tiny_general_tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d6924-b8a0-4e5b-a66d-e316e14a564d",
   "metadata": {
    "id": "3d4d6924-b8a0-4e5b-a66d-e316e14a564d"
   },
   "source": [
    "## 2.2 Generate text samples\n",
    "\n",
    "Here we will try  predicting/autocomplete  some text with the model. We will  set a prompt, instantiate a text streamer, and then have the model complete the prompt:  \n",
    "  \n",
    "**TextStreamer** captures tokens as they are generated by a  models generate() method. As new tokens are produced,   \n",
    "  they are **immediately converted to human-readable text** and pushed to the output (typically standard output).   \n",
    "**Advantage**: Instead of waiting for the entire output to be produced, it akllows to see the generated text in real time as the model produces each token.\n",
    "\n",
    " **model.generate()** method supports various parameters such as:\n",
    "\n",
    "1.  **max_new_tokens**: Determines how many tokens will be generated beyond the prompt.\n",
    "\n",
    "2.  **do_sample**: If set to True, it **enables sampling strategies for introducing randomness**; if False, it often resorts to greedy decoding.\n",
    "\n",
    "3.  **temperature**: Lower values (close to 0) make the output more deterministic, while higher values introduce more diversity in the generated text.\n",
    "\n",
    "4.  **repetition_penalty**: Helps reduce repetitiveness in the output by **penalizing repeated tokens**  \n",
    "\n",
    "5.  **do_sample=False**,   ensures that the model uses **greedy decoding**, where it always selects the token with the highest probability at each step.This makes the **output deterministic **\n",
    "\n",
    "**This flow is shown below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wwZB9d4yYgyb",
   "metadata": {
    "id": "wwZB9d4yYgyb"
   },
   "outputs": [],
   "source": [
    "image_path = os.path.join(path_to_scripts,\"images\",\"lesson1_img1.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a5efc-45b1-4d93-8fde-df8a102d08e1",
   "metadata": {
    "id": "804a5efc-45b1-4d93-8fde-df8a102d08e1"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "# set generate() params\n",
    " # Maximum number of tokens to generate\n",
    "max_new_tokens = 128\n",
    " # Use greedy decoding\n",
    "do_sample=False\n",
    "# The temperature parameter only applies when do_sample=True.\n",
    "temperature= 0.7  if  do_sample else None\n",
    "repetition_penalty=1.1\n",
    "\n",
    "\n",
    "# Tokenize input ( pt means Pytorch)\n",
    "model_inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\")\n",
    "# moves the model_inputs tensor to the same device where the model (tiny_general_model) is located.\n",
    "# By using tiny_general_model.device, you ensure that the input is moved to the correct GPU\n",
    "#if the model is on the GPU, or to the CPU if the model is on the CPU.\n",
    "model_inputs.to(tiny_general_model.device)\n",
    "# Configure text streaming\n",
    "streamer = TextStreamer(\n",
    "    # tokenizer to stream\n",
    "    tiny_general_tokenizer,\n",
    "     #exclude the original prompt from the output, displaying only the newly generated text.\n",
    "    skip_prompt=True,\n",
    "    #   Remove special tokens from output\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Generate text with specific parameters\n",
    "outputs = tiny_general_model.generate(\n",
    "    **model_inputs,\n",
    "    streamer=streamer,\n",
    "     # Enable caching for faster generation\n",
    "    use_cache=True,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=do_sample,\n",
    "   # temperature=temperature,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ez9gvj469VVk",
   "metadata": {
    "id": "Ez9gvj469VVk"
   },
   "source": [
    "## 2.3 Result:\n",
    "The model generates text that follows common patterns in natural language.  \n",
    "In this case, after prompt prompt = \"***I am an engineer. I love***\", it predicts phrases like \"***to travel and have a great time\"*** because these are common continuations in general language usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RaO1uFxX8hbM",
   "metadata": {
    "id": "RaO1uFxX8hbM"
   },
   "outputs": [],
   "source": [
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29640cf-3429-4e0c-be9c-a74833ddc29b",
   "metadata": {
    "id": "e29640cf-3429-4e0c-be9c-a74833ddc29b"
   },
   "source": [
    "## 3.1. Generate Python code completion  with same pretrained general model\n",
    "\n",
    "Use the model to write a python function called `find_max()` that finds the maximum value in a list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859db93d-c214-450b-8da3-57abfabf55b3",
   "metadata": {
    "id": "859db93d-c214-450b-8da3-57abfabf55b3"
   },
   "outputs": [],
   "source": [
    "# change the prompt\n",
    "prompt = \"def find_max(numbers):\"\n",
    "# rest of code is same\n",
    "model_inputs = tiny_general_tokenizer( prompt, return_tensors=\"pt\"\n",
    ").to(tiny_general_model.device)\n",
    "\n",
    "# streamer objects stays the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83f196-0118-4797-8894-c781a53c43a4",
   "metadata": {
    "id": "de83f196-0118-4797-8894-c781a53c43a4"
   },
   "outputs": [],
   "source": [
    "# Generate text with specific parameters\n",
    "outputs = tiny_general_model.generate(\n",
    "    **model_inputs,\n",
    "    streamer=streamer,\n",
    "     # Enable caching for faster generation\n",
    "    use_cache=True,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=do_sample,\n",
    "   # temperature=temperature,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vSB0AyCLa2U7",
   "metadata": {
    "id": "vSB0AyCLa2U7"
   },
   "source": [
    "## 3.2 Result\n",
    "*  The model  **fail miserably as model was trained on English, and not on code samples**\n",
    "* It generates some comments, but no calculations for finding max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5d693-9eb0-4394-8ad6-262ac5a48656",
   "metadata": {
    "id": "dcf5d693-9eb0-4394-8ad6-262ac5a48656"
   },
   "source": [
    "## 4.1. Generate Python code completion  with finetuned Python model\n",
    "\n",
    "* FineTuning involves Re-training the model on small amount of data, which is task specific.\n",
    "\n",
    "* Lets use  one such model from  the Hugging Face model library at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-code-instruct).\n",
    "\n",
    "*  This has been **fine tuned on Python code**\n",
    "*  Note that we will also use **corresponding tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf4752-c21d-44bd-8772-b1826d8f3bea",
   "metadata": {
    "id": "1aaf4752-c21d-44bd-8772-b1826d8f3bea"
   },
   "outputs": [],
   "source": [
    "model_name = \"TinySolar-248m-4k-code-instruct\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "# choose device map as CPU or Auto for GPU support\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load this model: Fine tuned\n",
    "tiny_finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    # Specify precision for efficiency\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the Fine tuned tokenizer by specifying just model path/name\n",
    "tiny_finetuned_tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b9233-c384-45ee-9fbb-cf39611b83bb",
   "metadata": {
    "id": "4a9b9233-c384-45ee-9fbb-cf39611b83bb"
   },
   "outputs": [],
   "source": [
    "# set the prompt for creating python method\n",
    "prompt =  \"def find_max(numbers):\"\n",
    "\n",
    "model_inputs = tiny_finetuned_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_finetuned_model.device)\n",
    "\n",
    "# Configure text streaming for finetuned model\n",
    "streamer_finetuned = TextStreamer(\n",
    "    # Fine tuned tokenizer to stream\n",
    "    tiny_finetuned_tokenizer,\n",
    "     #exclude the original prompt from the output\n",
    "    skip_prompt=True,\n",
    "    #   Remove special tokens from output\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Generate text with specific parameters\n",
    "outputs_fineTuned = tiny_finetuned_model.generate(\n",
    "    **model_inputs,\n",
    "    streamer=streamer_finetuned,\n",
    "     # Enable caching for faster generation\n",
    "    use_cache=True,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=do_sample,\n",
    "   # temperature=temperature,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mEv3zAGtd0yu",
   "metadata": {
    "id": "mEv3zAGtd0yu"
   },
   "source": [
    "## 4.2 Result:\n",
    "\n",
    "*************************\n",
    "*  There are some operations going on, but **we can have better results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8f283-71da-4736-9ac1-ba4ce9d5367f",
   "metadata": {
    "id": "6ee8f283-71da-4736-9ac1-ba4ce9d5367f"
   },
   "source": [
    "## 5.1 Generate Python code completion with Pre-Trained Python model\n",
    "\n",
    "* Here we will  use a version of **TinySolar-248m-4k** : **TinySolar-248m-4k-py** that has been further pretrained (**continued pretraining**) on a large selection  \n",
    "   of python code samples. You can find the model on Hugging Face at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7a061-ca41-41c3-8ffa-533a3f557d16",
   "metadata": {
    "id": "1df7a061-ca41-41c3-8ffa-533a3f557d16"
   },
   "outputs": [],
   "source": [
    "model_name = \"TinySolar-248m-4k-py\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "# choose device map as CPU or Auto for GPU support\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load this model: Fine tuned\n",
    "tiny_custom_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    # Specify precision for efficiency\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the tokenizer by specifing just model path/name\n",
    "tiny_custom_tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2240fd-19d6-4f44-89dc-9325f4fdc6b0",
   "metadata": {
    "id": "db2240fd-19d6-4f44-89dc-9325f4fdc6b0"
   },
   "outputs": [],
   "source": [
    "prompt = \"def find_max(numbers):\"\n",
    "\n",
    "model_inputs = tiny_custom_tokenizer(prompt, return_tensors=\"pt\").to(tiny_custom_model.device)\n",
    "\n",
    "# Configure text streaming for Pretrained model\n",
    "streamer_custom = TextStreamer(\n",
    "    # Fine tuned tokenizer to stream\n",
    "    tiny_custom_tokenizer,\n",
    "     #exclude the original prompt from the output\n",
    "    skip_prompt=True,\n",
    "    #   Remove special tokens from output\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Generate text with specific parameters\n",
    "outputs_Custom = tiny_custom_model.generate(\n",
    "    **model_inputs,\n",
    "    streamer=streamer_custom,\n",
    "     # Enable caching for faster generation\n",
    "    use_cache=True,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=do_sample,\n",
    "   # temperature=temperature,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec94120",
   "metadata": {
    "id": "4ec94120"
   },
   "source": [
    "## 5.2  Result :This is  much better\n",
    "**************************\n",
    "*Find the maximum number of numbers in a list.\"\"\"  \n",
    "   max = 0  \n",
    "   for num in numbers:  \n",
    "       if num > max:  \n",
    "           max = num  \n",
    "   return max*\n",
    "   ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FVTDbwYLjEvK",
   "metadata": {
    "id": "FVTDbwYLjEvK"
   },
   "outputs": [],
   "source": [
    "## Verify the function from prompt result\n",
    "def find_max(numbers):\n",
    "   max = 0\n",
    "   for num in numbers:\n",
    "       if num > max:\n",
    "           max = num\n",
    "   return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a767b-b5a1-4986-bef5-156a7e5a7acb",
   "metadata": {
    "id": "868a767b-b5a1-4986-bef5-156a7e5a7acb"
   },
   "outputs": [],
   "source": [
    "find_max([1,3,5,1,6,7,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vspyzd_LNhtt",
   "metadata": {
    "id": "vspyzd_LNhtt"
   },
   "outputs": [],
   "source": [
    "if run_remote  :\n",
    "  from google.colab import drive\n",
    "  drive.flush_and_unmount()\n",
    "  print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
