{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae54c1f5-8f4f-4fc3-a970-9d3e65d23355",
   "metadata": {
    "id": "ae54c1f5-8f4f-4fc3-a970-9d3e65d23355"
   },
   "source": [
    "# Lesson 4: Preparing Your Model for Training\r\n",
    "\r\n",
    "This lesson focuses on **configuring and initializing language models for training**, with an emphasis on weight initialization strategies and model architecture manipulation.\r\n",
    "\r\n",
    "## Key Objectives:\r\n",
    "1. **Environment Configuration**: Sets up Local or Remote Run with and Without GPU\r\n",
    "2. **Model Configuration**\r\n",
    "   - Use `LlamaConfig` to set up model architecture\r\n",
    "   - Customize parameters like hidden layers, hidden size, and attention heads\r\n",
    "3. **Weight Initialization Strategies**\r\n",
    "   - Random initialization\r\n",
    "   - Reusing pretrained model weights\r\n",
    "   - Downscaling from larger models\r\n",
    "   - Depth upscaling of smaller models\r\n",
    "4. **Model Manipulation Techniques**\r\n",
    "   - Layer removal for downscaling\r\n",
    "   - Layer duplication and concatenation for upscaling\r\n",
    "5. **Practical Implementation**\r\n",
    "   - Loading and saving models using Hugging Face's Transformers library\r\n",
    "   - Handling GPU allocation and memory management\r\n",
    "   - Tokenizer configuration and text generation\r\n",
    "6. **Transfer Learning Approaches**\r\n",
    "   - Continued pre-training\r\n",
    "   - Fine-tuning strategies\r\n",
    "\r\n",
    "## Next Steps:\r\n",
    "- Implement fine-tuning on task-specific datasets.\r\n",
    "- Explore advanced techniques like mixed-precision training and gradient checkpointing.\r\n",
    "ike mixed-precision training and gradient checkpointing\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3z6GF67hW94W",
   "metadata": {
    "id": "3z6GF67hW94W"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from typing import Tuple\n",
    "\n",
    "def setup_environment(curr_proj_folder: str = \"pretraining-llms\", google_drive_base_folder: str = \"Colab Notebooks\",\\\n",
    "                      run_remote: bool= True, use_gpu: bool = True) -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Sets up the environment for running code, handling local and remote execution.\n",
    "\n",
    "    Args:\n",
    "        curr_proj_folder (str, optional): Folder name of the current project. Defaults to \"pretraining-llms\".\n",
    "        google_drive_base_folder (str, optional): Folder name of the Google drive base folder. Defaults to \"\"Colab Notebooks\".\n",
    "        use_gpu (bool, optional): Whether to use GPU if available. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,bool]: (computed path_to_scripts,mount_success status)\n",
    "    \"\"\"\n",
    "    # Initialize mount status for Colab\n",
    "    mount_success = False\n",
    "    # Remote run code\n",
    "    if run_remote:\n",
    "      from google.colab import drive\n",
    "      # Mount Google Drive\n",
    "      drive.mount('/content/drive')\n",
    "      # Check if the mount was successful\n",
    "      if os.path.ismount('/content/drive'):\n",
    "        print(\"Google Drive mounted successfully!\")\n",
    "        mount_success = True\n",
    "      else:\n",
    "        print(\"Drive mount failed.\")\n",
    "      # By Default, this is complete mount path\n",
    "      mount_path = '/content/drive/MyDrive'\n",
    "\n",
    "      # complete path to current files\n",
    "      path_to_scripts = os.path.join(mount_path, google_drive_base_folder,curr_proj_folder)\n",
    "      # Create the directory if it doesn't exist\n",
    "      if not os.path.exists(path_to_scripts):\n",
    "        os.makedirs(path_to_scripts)\n",
    "        # change to the path\n",
    "      os.chdir(path_to_scripts)\n",
    "      print(f'Running code in path {os.getcwd()}')\n",
    "    # Local Run\n",
    "    else:\n",
    "      path_to_scripts  = os.getcwd()\n",
    "      # folder name provided as argument should match the one existing\n",
    "      assert os.path.basename(path_to_scripts ) == curr_proj_folder, \\\n",
    "          f\"Folder Name Mismatch: {os.path.basename(path_to_scripts )} != {curr_proj_folder}\"\n",
    "      print(f'Running code in path {path_to_scripts }')\n",
    "    # check GPU usage\n",
    "    if use_gpu:\n",
    "      try:\n",
    "        gpu_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "        print(\"******GPU is available and will be used:**********\")\n",
    "        print(gpu_info)\n",
    "      except subprocess.SubprocessError:\n",
    "        print(\"GPU check failed (nvidia-smi not found or no GPU available). Falling back to CPU.\")\n",
    "        use_gpu = False  # Force CPU usage if GPU check fails\n",
    "    else:\n",
    "        print(\"******use_gpu is set to False. Using CPU******\")\n",
    "    return  path_to_scripts,mount_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872653d-0363-4d33-803b-192f05b70ad2",
   "metadata": {},
   "source": [
    "Always set following parameters as needed before each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GFdAziCbVhP9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFdAziCbVhP9",
    "outputId": "6c516d66-88f3-40ac-a79d-a0d21eef126d"
   },
   "outputs": [],
   "source": [
    "# Project-specific configuration parameters\n",
    "# Specifies the current project folder name\n",
    "curr_proj_folder = \"pretraining-llms\"\n",
    "# Base folder name in Google Drive where notebooks are stored\n",
    "google_drive_base_folder = \"Colab Notebooks\"\n",
    "# Flag to determine whether to use GPU for computations\n",
    "use_gpu = True\n",
    "# Flag to indicate remote execution environment\n",
    "run_remote = False\n",
    "# Flag to control model loading from a specific folder or through URL\n",
    "load_model_from_folder = False\n",
    "\n",
    "if run_remote:\n",
    "  run_local = False\n",
    "  run_local_usingColab = False\n",
    "else:\n",
    "  run_local = False\n",
    "  run_local_usingColab = not run_local\n",
    "\n",
    "# call method to setup environment\n",
    "path_to_scripts,mount_success = setup_environment(curr_proj_folder = curr_proj_folder, \\\n",
    "                                   google_drive_base_folder =  google_drive_base_folder,\\\n",
    "                                    run_remote = run_remote, use_gpu = use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1SSZPmnyUqeW",
   "metadata": {
    "id": "1SSZPmnyUqeW"
   },
   "source": [
    "### Decoder Only Transformer Architecture\n",
    "\n",
    "* Decoder-only architecture is used in models like Llama and is an Autoregrresive Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YBvIYKA3VEhQ",
   "metadata": {
    "id": "YBvIYKA3VEhQ"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "image_path = os.path.join(path_to_scripts,\"images\",\"decoderOnly.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wo3mvheyaavs",
   "metadata": {
    "id": "wo3mvheyaavs"
   },
   "source": [
    "Each layer processes the input sequence and passes information to the next layer.The final layer produces output probabilities for the next token\n",
    "\n",
    "\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **Stack of Decoder Blocks**:  \n",
    "  The model is composed of multiple stacked decoder layers.\n",
    "\n",
    "Each decoder block contains the following elements:\n",
    "\n",
    "1. **Linear Transformations**:  \n",
    "   Used to project input data into different feature spaces for better representation.\n",
    "\n",
    "2. **Self-attention Mechanism**:  \n",
    "   - Computes attention scores to focus on relevant parts of the input.\n",
    "   - Uses masked self-attention to prevent looking at future tokens, ensuring autoregressive behavior.\n",
    "\n",
    "3. **Layer Normalization**:  \n",
    "   - Stabilizes training and speeds up convergence by normalizing activations.\n",
    "\n",
    "4. **Feed-forward Layers**:  \n",
    "   - Consist of fully connected layers that apply non-linear transformations to enhance the model’s expressiveness.\n",
    "\n",
    "\n",
    "5.  **Classifier Layer**:  \n",
    "  The final output from the last decoder block is passed through a classifier layer to generate most probable token predictions.\n",
    "\n",
    "- **Number of Layers (n)**:  \n",
    "  The architecture can have multiple decoder blocks, where \"n\" determines the depth of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r9fB51lQgPRO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "id": "r9fB51lQgPRO",
    "outputId": "8061e8e4-5a6d-4ddf-8f85-d9176a9b79cd"
   },
   "outputs": [],
   "source": [
    "image_path = os.path.join(path_to_scripts,\"images\",\"solar10_7_B_Pretrain.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lqAzWU_cgdgJ",
   "metadata": {
    "id": "lqAzWU_cgdgJ"
   },
   "source": [
    "* Here, starting point was English language  SOLAR model , and it was **pretrained  for Koren Language extension**\n",
    "* **The 200B tokens were a mix of Korean and English language tokens**\n",
    "* the price is 0.2M, expensive but much cheaper than Training from scratch\n",
    "* **number of parameters is 10.7B, which increased from 7B English language model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c998c-7135-4a0f-b3fa-9d2147f64eb6",
   "metadata": {
    "height": 251,
    "id": "eb5c998c-7135-4a0f-b3fa-9d2147f64eb6"
   },
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecation warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Fix random seed for reproducibility across all PyTorch operations\n",
    "    Args:\n",
    "    seed (int): Seed value to use for random number generation\n",
    "    \"\"\"\n",
    "    # Set seed for CPU-based random number generation\n",
    "    # Affects:\n",
    "    # - Initial weight randomization in model layers\n",
    "    # - Data shuffling operations\n",
    "    # - Any CPU-based random number generation in PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for CUDA (GPU) random number generation\n",
    "    # Important for:\n",
    "    # - GPU-based dropout layers\n",
    "    # - GPU-accelerated matrix operations with randomness\n",
    "    # - Ensures reproducibility when using CUDA-enabled devices\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # Force cuDNN to use deterministic algorithms\n",
    "    # Tradeoffs:\n",
    "    # - May slightly reduce performance (~10-20%)\n",
    "    # - Ensures reproducibility in convolution operations\n",
    "    # - Required for exact reproducibility of results\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Disable cuDNN benchmarking optimization\n",
    "    # Why:\n",
    "    # - Benchmarking automatically selects fastest algorithms\n",
    "    # - Different algorithms may be selected across runs\n",
    "    # - Disabling ensures consistent algorithm selection\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Initialize the seed configuration with default seed value\n",
    "# Note: Must be called before any model initialization or data loading\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f9486-36ce-4361-9f0b-5bd2f190c332",
   "metadata": {
    "id": "582f9486-36ce-4361-9f0b-5bd2f190c332"
   },
   "source": [
    "## 1. Model configuration\n",
    "\n",
    "*  Configure models based on Meta's Llama family.\n",
    "\n",
    "Let's create a `LlamaConfig` object to configure the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e85be0c-d2fa-4070-9450-fce9ec7b5312",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 64,
    "id": "1e85be0c-d2fa-4070-9450-fce9ec7b5312",
    "outputId": "1ec8ae0e-0aec-4761-92f3-857307e170a6"
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig\n",
    "config = LlamaConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f440840",
   "metadata": {
    "id": "1f440840"
   },
   "source": [
    "let's update parameters to create a smaller, more manageable model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38993554-f050-479d-9f65-846f935af606",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 132,
    "id": "38993554-f050-479d-9f65-846f935af606",
    "outputId": "16b7687b-0d62-4ae9-e793-18e4c3d3807a"
   },
   "outputs": [],
   "source": [
    "config.num_hidden_layers = 12      # reduced from 32 to 12\n",
    "config.hidden_size = 1024          # reduced 1/4 from 4096 to 1024\n",
    "config.intermediate_size = 4096    # reduced 1/3 from 11008 to 4096 (dimension of MLP representations)\n",
    "config.num_key_value_heads = 8     # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)\n",
    "config.torch_dtype = \"bfloat16\"    # for half-precision training\n",
    "config.use_cache = False           # `True` is incompatible w/ gradient checkpointing\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb71119-897d-4b0a-91fb-4d49decd5f50",
   "metadata": {
    "id": "4bb71119-897d-4b0a-91fb-4d49decd5f50"
   },
   "source": [
    "## 2. Weight initialization\n",
    "\n",
    "Ther are  four different ways to initialize the weights of a model for training\n",
    "1. **Random weight initialization**\n",
    "2. **Using an existing model** for continued pre-training\n",
    "3. **Downscaling** an existing model  \n",
    "4. **Upscaling** an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d47054-897d-4ab9-a382-6ea1aa824b88",
   "metadata": {
    "id": "b1d47054-897d-4ab9-a382-6ea1aa824b88"
   },
   "source": [
    "### 2.1 Random weight initialization\n",
    "\n",
    "* Sets all weights to values from a **truncated normal distribution** with mean 0 and standard deviation of 0.02.\n",
    "* Values beyond 2-sigma from the mean are set to 0. ( *+-2Sigma*)\n",
    "*  We will import and use **LlamaForCausalLM:** **Class specifically designed for autoregressive text generation**\n",
    "\n",
    "**NOTE:**\n",
    "Local O/p shows:\n",
    "\n",
    "LlamaAttention with projection dimensions:\n",
    "\n",
    "q_proj: 1024 → 4096\n",
    "\n",
    "k_proj: 1024 → 1024\n",
    "\n",
    "v_proj: 1024 → 1024\n",
    "\n",
    "o_proj: 4096 → 1024\n",
    "\n",
    "**DeepLearning.AI course** output  shows:\n",
    "\n",
    "LlamaSdpaAttention with projection dimensions:\n",
    "\n",
    "q_proj: 1024 → 1024\n",
    "\n",
    "k_proj: 1024 → 256\n",
    "\n",
    "v_proj: 1024 → 256\n",
    "\n",
    "o_proj: 1024 → 1024\n",
    "This difference is likely due to version changes in the **Transformers library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754ad49-4ab4-4768-8140-c00cc73a6933",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 115,
    "id": "b754ad49-4ab4-4768-8140-c00cc73a6933",
    "outputId": "8c165971-3d42-4c0e-98eb-db2183746cb6"
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "# Use the LlamaConfig object defined above to create  architecture\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "# set model to be run on GPU if needed and precision as in \n",
    "# Manual device/dtype configuration\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "# Cast weights to bfloat16\n",
    "model = model.to(torch.bfloat16)  \n",
    "print(model)\n",
    "def print_nparams(model):\n",
    "    \"\"\"\n",
    "    Calculate the total number of model parameters\n",
    "    Args:\n",
    "        model: The PyTorch model to analyze\n",
    "    Returns:\n",
    "        None, prints the total parameter count\n",
    "    \"\"\"\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"The total number of parameters is: {nparams}\")\n",
    "print_nparams(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58183c9f",
   "metadata": {
    "id": "58183c9f"
   },
   "source": [
    "Show sample of the weights in a single layer:\n",
    "* In transformer models like Llama, the **self-attention mechanism projects input embeddings into query (q), key (k), and value (v) representations**. The **q_proj** specifically handles the transformation of input embeddings into query vectors that are used to determine which parts of the sequence the model should focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2350b11-b989-4b7b-8f60-0b53d6748b26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 132,
    "id": "b2350b11-b989-4b7b-8f60-0b53d6748b26",
    "outputId": "de2a7f8b-f4be-47a8-bc6f-ab97aab65d29"
   },
   "outputs": [],
   "source": [
    "# from the LLama model layers , choose layer 0, and extract the weight parameters\n",
    "# for Query Projection in the self attention mechanism\n",
    "layer_name = \"model.layers.0.self_attn.q_proj.weight\"\n",
    "\n",
    "# go through each layer, and extract the one chosen\n",
    "for name, param in model.named_parameters():\n",
    "    if name == layer_name:\n",
    "        print(f\"First 30 weights of layer '{layer_name}':\")\n",
    "        print(param.data.view(-1)[:30])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833896a8",
   "metadata": {
    "id": "833896a8"
   },
   "source": [
    "Use this  model for inference with a randomly initialized weights as above.  \n",
    "**Result: Random gibberish  op as model  is not trained yet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f1e5b-ad7a-429d-9268-3dc1abadc881",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 455,
    "id": "bb7f1e5b-ad7a-429d-9268-3dc1abadc881",
    "outputId": "e5aed958-38c2-4eec-8313-e2457b3c8cfd"
   },
   "outputs": [],
   "source": [
    "# Load a tokenizer from Upstage Solar,# which is compatible with  the Llama-2 tokenizer\n",
    "model_name = \"SOLAR-10.7B-v1.0\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path_or_name)\n",
    "# Run simple inference with prompt\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# set generate() params\n",
    " # Maximum number of tokens to generate\n",
    "max_new_tokens = 128\n",
    " # Use greedy decoding\n",
    "do_sample=False\n",
    "# The temperature parameter only applies when do_sample=True.\n",
    "temperature= 0.7  if  do_sample else None\n",
    "repetition_penalty=1.1\n",
    "\n",
    "prompt = \"I am an engineer. I love\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Configure text streaming for the  model\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Generate text with specific parameters\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=do_sample,\n",
    "    temperature=temperature,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b769aaf",
   "metadata": {
    "id": "5b769aaf"
   },
   "source": [
    "Remove the model from memory to avoid crashing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc8e08-f68c-422f-96dd-d0515ab00f29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 115,
    "id": "6dfc8e08-f68c-422f-96dd-d0515ab00f29",
    "outputId": "d6fc5cc1-fb8b-45e5-c264-a43e3a2680b6"
   },
   "outputs": [],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d55a00-3402-48a7-8bce-81d8a546e213",
   "metadata": {
    "id": "e6d55a00-3402-48a7-8bce-81d8a546e213"
   },
   "source": [
    "### Reuse general pretrained model weights\n",
    "\n",
    "Load an existing Pre-trained model ( **Whose weights were Retrained from random**) and use it as-is  or\n",
    "**Retrain with new Data : Continued Pre-Training**\n",
    "\n",
    "The op is\n",
    "\n",
    "```\n",
    "to travel and have a great time, but I'm not sure if I can do it all again.\n",
    "I've been working on my first book for the last 10 years, and I've always  \n",
    " wanted to write about something that has happened in my life.             \n",
    "  It's been a long journey, but I've finally found my voice.  I've written a lot of books, and I've had some really good ones.             \n",
    "   I've also written a few short stories, and I've done a                  \n",
    "    couple of short story collections.                                       \n",
    "             I've also written a few short stories, and I\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a27a2-e56c-406f-bf2a-086f6c01a703",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 149,
    "id": "2b8a27a2-e56c-406f-bf2a-086f6c01a703",
    "outputId": "004c95fa-5c05-4d9e-cf91-e751f4852d4f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_name = \"TinySolar-248m-4k\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path_or_name)\n",
    "\n",
    "# Run simple inference with prompt\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Generate text with specific parameters\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "     # Enable caching for faster generation\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36ced3",
   "metadata": {
    "id": "0d36ced3"
   },
   "source": [
    "Remove the model from memory to avoid crashing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa497e-c35f-459b-9000-0b8d4c601491",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 64,
    "id": "dffa497e-c35f-459b-9000-0b8d4c601491",
    "outputId": "2bae316f-6828-4903-99e9-cd7efa9fb091"
   },
   "outputs": [],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cf37c-f8c6-45df-a7e2-95a8db9f1b93",
   "metadata": {
    "id": "e91cf37c-f8c6-45df-a7e2-95a8db9f1b93"
   },
   "source": [
    "### Downscaling from a general pretrained model\n",
    "\n",
    "**Downscale :  Remove layers usually in the middle and Retrain**\n",
    "Example is shown below: Remove 2 layers from tinySolar-248m-4k model , 12 layer model to a 10 layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hgiiiyNTAz8n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "id": "hgiiiyNTAz8n",
    "outputId": "5c466ffa-76e6-47e6-cde4-2f07401cc510"
   },
   "outputs": [],
   "source": [
    "image_path = os.path.join(path_to_scripts,\"images\",\"downScale_model.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AlbIAEo0CWrs",
   "metadata": {
    "id": "AlbIAEo0CWrs"
   },
   "source": [
    "###  Why Remove Layers from the Middle?\n",
    "1.  Middle Layers Are Less Specialized:In transformer architectures, the **lower layers tend to extract basic features (e.g., syntax)**, while the upper layers focus on task-specific or high-level features (e.g., semantics).\n",
    "\n",
    "2.  Middle layers often act as intermediaries and are less specialized, making them good candidates for removal **without significantly impacting performance.**\n",
    "\n",
    "Preserve Model Functionality:\n",
    "\n",
    "By keeping the first few and last few layers intact, t**he model retains its ability to process input embeddings (early layers) and generate meaningful outputs (later layers).**\n",
    "\n",
    "**Why Retrain After Downscaling?**\n",
    "\n",
    "* **Restore Coherence/Same performance:**\n",
    "The remaining parameters **need to adjust to compensate for the removed layers**.Retraining helps align weights across layers for smooth forward propagation.\n",
    "\n",
    "* **Adapt to New Architecture:** The reduced model architecture may require fine-tuning on a large corpus of text so that it can re-learn intermediate representations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ac9d5-a5bd-42b4-bdad-c1954f98da87",
   "metadata": {
    "height": 166,
    "id": "404ac9d5-a5bd-42b4-bdad-c1954f98da87"
   },
   "outputs": [],
   "source": [
    "# load TinySolar-248m-4k model and corresponding toeknizer again\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "model_name = \"TinySolar-248m-4k\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32988c2-07ad-4858-ad4d-336a1dda3be5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "e32988c2-07ad-4858-ad4d-336a1dda3be5",
    "outputId": "2121bfb8-6e8c-4506-ac28-1b332762cbd6"
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "print_nparams(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784dfa5",
   "metadata": {
    "id": "0784dfa5"
   },
   "source": [
    "Remove the middle two layers (layers 5 and 6) and update the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e3016-dd42-4a5d-aa2e-ba203c9bb745",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 183,
    "id": "130e3016-dd42-4a5d-aa2e-ba203c9bb745",
    "outputId": "35abe37d-b6eb-4cf4-ec9b-aa3a570626ed"
   },
   "outputs": [],
   "source": [
    "# extract layers\n",
    "layers = model.model.layers\n",
    " # Keep first 5 [0:4] and last 5 layers\n",
    "model.model.layers = layers[:5] + layers[-5:]\n",
    "\n",
    "#Respecify the configuraion\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    num_hidden_layers=len(model.model.layers),\n",
    ")\n",
    "model.config = config\n",
    "\n",
    "print_nparams(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ea7cd",
   "metadata": {
    "id": "bc3ea7cd"
   },
   "source": [
    "Clear the memory to avoid crashing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa3dae-e6dd-4892-8965-b0a03d987456",
   "metadata": {
    "height": 81,
    "id": "35fa3dae-e6dd-4892-8965-b0a03d987456"
   },
   "outputs": [],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54991d43-488d-4474-be1a-db0531961672",
   "metadata": {
    "id": "54991d43-488d-4474-be1a-db0531961672"
   },
   "source": [
    "### Depth Upscaling from a general pretrained model\n",
    "\n",
    "**Depth Upscaling:** involves increasing the number of layers in a pretrained model while preserving and leveraging the knowledge from the original model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kxpc27ZIFtNZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "kxpc27ZIFtNZ",
    "outputId": "287c20a3-ee0b-4e45-f1cf-995b177e7de1"
   },
   "outputs": [],
   "source": [
    "image_path = os.path.join(path_to_scripts,\"images\",\"depth_upscaling.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oxA47BxAFVvu",
   "metadata": {
    "id": "oxA47BxAFVvu"
   },
   "source": [
    "###  How the Process Works\n",
    "* **Start with a base model**, like Model A **with  pretrained weights** and **N(4 here) layers**\n",
    "* **Duplicate this model** and then N1(1 here)layers are removed from the end of the original base model and N1 (1 here) from the beginning of the duplicate. This process results in two N-N1 (4-1=3) layer models.\n",
    "\n",
    "* Concatanate these models, forming a new model with 2*(N-N1) layers.\n",
    "* By removing N1 layers from each part, the **'middle' layers of the upscaled model are effectively discarded.** This strategy **reduces the layer distance at the seam**, instead of directly connecting layer N to layer 1, a**s would be the case in a simple duplication method**\n",
    "\n",
    "* In The second stage called **continued pre-training**,   further pretraining the scaled model to **recover and potentially surpass the performance of the base LLM.**\n",
    "\n",
    "###Why Depth Upscaling?\n",
    "1.  **Increase Model Capacity:** Adding more layers allows the model to capture more complex patterns and representations.\n",
    "\n",
    "2.  **Leverage pretrained Knowledge:** Instead of training a larger model from scratch, you can **reuse and expand** an existing smaller model.\n",
    "\n",
    "3.  **Efficient Transfer Learning:** By copying weights from a smaller pretrained model, it **reduces the amount of training required** for the larger model.\n",
    "\n",
    "### Compariosn with MIxture of Experts\n",
    "1.  Methods like  **mixture-of-experts (MoE)**, may involve more intricate modifications to the model architecture, **including the introduction of expert layers and gating mechanisms** which can make **training and integration more challenging.**\n",
    "\n",
    "2.  **DUS**  excels in **capturing long-range dependencies and handling complex representations** although it may incur **higher computational costs** and the risk of **overfitting**. On the other hand, MOE **employs a gating network to dynamically allocate specific \"sub-experts\" for different inputs, enhancing efficiency and robustness while reducing computational demands**. However, it may not be as effective for tasks requiring broad global context.\n",
    "\n",
    "Here you are going to upscale the tinySolar-248m-4k model from 12 layers to 16 layers. Here are the steps we 'll take:\n",
    "1. Configure a 16 layer model and initialize it with random weights\n",
    "2. Load the 12 layer tinySolar-248m-4k model into memory\n",
    "3. Copy the bottom 8 and top 8 layers from the 12 layer model and use them to overwrite the random weights of the 16 layer model\n",
    "4. Copy over the embedding and classifying layers to replace the randomly initialized counterparts in the 16 layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3sFeBxCLvEx",
   "metadata": {
    "id": "b3sFeBxCLvEx"
   },
   "source": [
    "### Example Steps in Depth Upscaling\n",
    "1. ***Configure a Larger Model with Random Weights***.\n",
    "Start by creating a configuration for the larger model (in this case, 16 layers).Initialize the new model with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472958a-7243-46f6-bd6c-6787286a819a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 183,
    "id": "0472958a-7243-46f6-bd6c-6787286a819a",
    "outputId": "51c99cd2-77ce-4681-a77d-aea8b3bf574d"
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "# Create configuration for a 16-layer model\n",
    "config = LlamaConfig(\n",
    "    num_hidden_layers=16,  # Target number of layers\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=4096,\n",
    "    num_attention_heads=32,\n",
    "    num_key_value_heads=8,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "# Initialize the larger model with random weights\n",
    "upscaled_model = LlamaForCausalLM(config)\n",
    "upscaled_model = upscaled_model.to(dtype=torch.bfloat16)\n",
    "print_nparams(upscaled_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3BGuHD1MXfS",
   "metadata": {
    "id": "a3BGuHD1MXfS"
   },
   "source": [
    " 2. ***Load the Pretrained Smaller Model***: Load the pretrained smaller model (12 layers) that you want to upscale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b3dbe-3aa4-4858-b9d3-d59645108056",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 166,
    "id": "685b3dbe-3aa4-4858-b9d3-d59645108056",
    "outputId": "84d9ba28-6039-4241-a6c9-bb794606b56d"
   },
   "outputs": [],
   "source": [
    "model_name = \"TinySolar-248m-4k\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "print_nparams(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KbBteQeFM750",
   "metadata": {
    "id": "KbBteQeFM750"
   },
   "source": [
    "3.  ***Copy Layers from Pretrained Model to Larger Model***  \n",
    "Specifically:\n",
    "\n",
    "* Use the first few layers and last few layers from the smaller model.\n",
    "\n",
    "* use them to overwrite the random weights of the Larger  model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaaefbf-d73a-4ed0-bac6-b92575012952",
   "metadata": {
    "height": 183,
    "id": "acaaefbf-d73a-4ed0-bac6-b92575012952"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Combine first 8 and last 8 layers from\n",
    "#the pretrained 12-layer model\n",
    "upscaled_model.model.layers = deepcopy(pretrained_model.model.layers[:8]) + \\\n",
    "                     deepcopy(pretrained_model.model.layers[4:])\n",
    "#This creates a 16-layer model as:\n",
    "#A1, A2, A3, A4, A5, A6, A7, A8, A5, A6, A7, A8, A9, A10, A11, A12\n",
    "\n",
    "# Copy embedding layer (shared across all layers)\n",
    "upscaled_model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n",
    "\n",
    "# Copy language modeling head (final output layer)\n",
    "upscaled_model.lm_head = deepcopy(pretrained_model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdfdb4",
   "metadata": {
    "id": "aebdfdb4"
   },
   "source": [
    "Check the number of parameters is still Same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dffe64-8451-4146-9c5d-cdfd43e43230",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "54dffe64-8451-4146-9c5d-cdfd43e43230",
    "outputId": "fab0efb6-169c-49b3-b9a2-a7843fd8f52d"
   },
   "outputs": [],
   "source": [
    "print_nparams(upscaled_model)  # 308839424 => 308M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df68e10",
   "metadata": {
    "id": "0df68e10"
   },
   "source": [
    " 4. **Fine-Tune or Retrain on New Data**\n",
    "The upscaled model now needs to be fine-tuned or retrained on a large corpus of text to adapt its weights and fully utilize its increased capacity.\n",
    "\n",
    "*Example Output*\n",
    "Before fine-tuning, we can test the upscaled model with simple inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8a453-96cd-4f6c-91c5-5e529b2e41ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 319,
    "id": "f8a8a453-96cd-4f6c-91c5-5e529b2e41ce",
    "outputId": "9db7345c-5f7c-4473-b48e-53a9f4bf9e26"
   },
   "outputs": [],
   "source": [
    "# Run simple inference to show no trained model\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(upscaled_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "# Move the upscaled_model to the same device as the inputs\n",
    "upscaled_model = upscaled_model.to(inputs.input_ids.device)\n",
    "\n",
    "outputs = upscaled_model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1814e0d",
   "metadata": {
    "id": "f1814e0d"
   },
   "source": [
    "### Save the model to disk\n",
    "\n",
    "Note the new model name here which reflects the 308 million parameters of the new, upscaled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31215375-a0bd-4f9d-8ea2-1385c315c25b",
   "metadata": {
    "height": 30,
    "id": "31215375-a0bd-4f9d-8ea2-1385c315c25b"
   },
   "outputs": [],
   "source": [
    "save_dir = \".//savedModel\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_path = os.path.join(save_dir,\"TinySolar-308m-4k-init\").replace('\\\\', '//') \n",
    "upscaled_model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1270014-a3d9-4e8b-93cf-c5a08da7e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if model files were created sucessfully\n",
    "def check_files_in_folder(folder_path):\n",
    "    # Get list of files in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    # Check if folder is empty\n",
    "    if not files:\n",
    "        print(f\"The folder '{folder_path}' is empty.\")\n",
    "        return False\n",
    "    \n",
    "    # Print filenames\n",
    "    print(f\"Files found in '{folder_path}':\")\n",
    "    for file in files:\n",
    "        print(f\"- {file}\")\n",
    "    \n",
    "    return True\n",
    "check_files_in_folder(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a86ce2-8747-4ad4-a7eb-75875fcb0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_remote  :\n",
    "  from google.colab import drive\n",
    "  drive.flush_and_unmount()\n",
    "  print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
