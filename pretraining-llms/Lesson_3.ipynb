{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93b5be4-1d2f-4f81-b603-dc5317dc9922",
   "metadata": {
    "id": "e93b5be4-1d2f-4f81-b603-dc5317dc9922"
   },
   "source": [
    "# Lesson 3: LLM Data Packaging Tutorial: From Tokens to Training Sequences\r\n",
    "\r\n",
    "This notebook demonstrates the complete workflow for preparing tokenized data for LLM training, focusing on efficient sequence packing:\r\n",
    "\r\n",
    "## Core Processes:\r\n",
    "**Environment Configuration**: Sets up Local or Remote Run with and without GPU\r\n",
    "\r\n",
    "**Data Preparation**:\r\n",
    "- Loads preprocessed JSON data\r\n",
    "- Implements dataset sharding (10 shards)\r\n",
    "- Uses Hugging Face `datasets` library\r\n",
    "\r\n",
    "**Tokenization Workflow**:\r\n",
    "- Initializes SOLAR-10.7B tokenizer\r\n",
    "- Implements BOS/EOS token insertion\r\n",
    "- Processes text through parallel mapping\r\n",
    "- Analyzes token distributions (millions per shard)\r\n",
    "\r\n",
    "**Sequence Packaging**:\r\n",
    "- Concatenates all token IDs\r\n",
    "- Sets max sequence length (32 for demo)\r\n",
    "- Reshapes into fixed-length sequences\r\n",
    "- Handles length divisibility constraints\r\n",
    "\r\n",
    "## Key Technical Components:\r\n",
    "**Special Tokens**: Adds beginning/end markers  \r\n",
    "**Memory Optimization**:\r\n",
    "- Numpy array manipulation\r\n",
    "- Sequence length vs batch size tradeoffs  \r\n",
    "\r\n",
    "**Efficiency Features**:\r\n",
    "- Dataset sharding for manageable processing\r\n",
    "- Cached mapping operations\r\n",
    "- GPU availability checks\r\n",
    "\r\n",
    "## Output Artifacts:\r\n",
    "- Packaged dataset in Hugging Face format\r\n",
    "- JSON serialization of token sequences\r\n",
    "- Complete pipeline from raw text to training-ready batches\r\n",
    "\r\n",
    "## Visual Guides Included:\r\n",
    "1. Tokenization process diagram  \r\n",
    "2. Sequence packing visual helps the**model understand sequence boundaries.**\r\n",
    "*model understand sequence boundaries.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bUIs8HrbKU-A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUIs8HrbKU-A",
    "outputId": "1a541514-dc9f-4d22-938f-cf9cf831d000"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from typing import Tuple\n",
    "\n",
    "def setup_environment(curr_proj_folder: str = \"pretraining-llms\", google_drive_base_folder: str = \"Colab Notebooks\",\\\n",
    "                      run_remote: bool= True, use_gpu: bool = True) -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Sets up the environment for running code, handling local and remote execution.\n",
    "\n",
    "    Args:\n",
    "        curr_proj_folder (str, optional): Folder name of the current project. Defaults to \"pretraining-llms\".\n",
    "        google_drive_base_folder (str, optional): Folder name of the Google drive base folder. Defaults to \"\"Colab Notebooks\".\n",
    "        use_gpu (bool, optional): Whether to use GPU if available. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str,bool]: (computed path_to_scripts,mount_success status)\n",
    "    \"\"\"\n",
    "    # Initialize mount status for Colab\n",
    "    mount_success = False\n",
    "    # Remote run code\n",
    "    if run_remote:\n",
    "      from google.colab import drive\n",
    "      # Mount Google Drive\n",
    "      drive.mount('/content/drive')\n",
    "      # Check if the mount was successful\n",
    "      if os.path.ismount('/content/drive'):\n",
    "        print(\"Google Drive mounted successfully!\")\n",
    "        mount_success = True\n",
    "      else:\n",
    "        print(\"Drive mount failed.\")\n",
    "      # By Default, this is complete mount path\n",
    "      mount_path = '/content/drive/MyDrive'\n",
    "\n",
    "      # complete path to current files\n",
    "      path_to_scripts = os.path.join(mount_path, google_drive_base_folder,curr_proj_folder)\n",
    "      # Create the directory if it doesn't exist\n",
    "      if not os.path.exists(path_to_scripts):\n",
    "        os.makedirs(path_to_scripts)\n",
    "        # change to the path\n",
    "      os.chdir(path_to_scripts)\n",
    "      print(f'Running code in path {os.getcwd()}')\n",
    "    # Local Run\n",
    "    else:\n",
    "      path_to_scripts  = os.getcwd()\n",
    "      # folder name provided as argument should match the one existing\n",
    "      assert os.path.basename(path_to_scripts ) == curr_proj_folder, \\\n",
    "          f\"Folder Name Mismatch: {os.path.basename(path_to_scripts )} != {curr_proj_folder}\"\n",
    "      print(f'Running code in path {path_to_scripts }')\n",
    "    # check GPU usage\n",
    "    if use_gpu:\n",
    "      try:\n",
    "        gpu_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "        print(\"******GPU is available and will be used:**********\")\n",
    "        print(gpu_info)\n",
    "      except subprocess.SubprocessError:\n",
    "        print(\"GPU check failed (nvidia-smi not found or no GPU available). Falling back to CPU.\")\n",
    "        use_gpu = False  # Force CPU usage if GPU check fails\n",
    "    else:\n",
    "        print(\"******use_gpu is set to False. Using CPU******\")\n",
    "    return  path_to_scripts,mount_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e001469-493d-491e-926a-ce06b32ea806",
   "metadata": {},
   "source": [
    "Always set following parameters before each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2481205-edf0-42b4-8325-df400fa49752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project-specific configuration parameters\n",
    "# Specifies the current project folder name\n",
    "curr_proj_folder = \"pretraining-llms\"\n",
    "# Base folder name in Google Drive where notebooks are stored\n",
    "google_drive_base_folder = \"Colab Notebooks\"\n",
    "# Flag to determine whether to use GPU for computations\n",
    "use_gpu = True\n",
    "# Flag to indicate remote execution environment\n",
    "run_remote = False\n",
    "# Flag to control model loading from a specific folder or through URL\n",
    "load_model_from_folder = False\n",
    "\n",
    "\n",
    "if run_remote:\n",
    "  run_local = False\n",
    "  run_local_usingColab = False\n",
    "else:\n",
    "  run_local = False\n",
    "  run_local_usingColab = not run_local\n",
    "\n",
    "# call method to setup environment\n",
    "path_to_scripts,mount_success = setup_environment(curr_proj_folder = curr_proj_folder, \\\n",
    "                                   google_drive_base_folder =  google_drive_base_folder,\\\n",
    "                                    run_remote = run_remote, use_gpu = use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5fc82-d803-40d3-a8c4-a8def716afd2",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "LLMs don't process raw text directly - they work with numerical representations called tokens. Before training an LLM, we need to:\n",
    "\n",
    " **Tokenize our text data**: convert text into numbers using a specific tokenizer\n",
    "\n",
    " **Pack these tokens** into fixed-length sequences for efficient training\n",
    "\n",
    "This process is crucial because:\n",
    "\n",
    "1.   **Transforms** human-readable text into machine-processable numbers.\n",
    "2.   **Ensures** all training examples have** consistent dimensions.**\n",
    "3.   **Optimizes memory usage** during training.\n",
    "4.   **Adds special tokens** to help the **model understand sequence boundaries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hqyMTWV-oHAR",
   "metadata": {
    "editable": true,
    "id": "hqyMTWV-oHAR",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.1 Loading and Preparing the Dataset\n",
    "First, we'll load our preprocessed dataset and examine a small portion of it to make our notebook run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd6984-9ef5-49ef-97f0-3bf6d61ab5ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 149,
    "id": "42fd6984-9ef5-49ef-97f0-3bf6d61ab5ec",
    "outputId": "1a0f2ceb-eec2-4182-d14e-2433d3d9e539"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "# preTrained data path\n",
    "load_dir = \".//saved_pretrain_cleaned_data\"\n",
    "file_load_path = os.path.join(load_dir + \"//preprocessed_dataset.json\")\n",
    "# Load the dataset from a json  file\n",
    "dataset = datasets.load_dataset(\n",
    "    \"json\",  data_files=file_load_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26b86b",
   "metadata": {
    "id": "6c26b86b"
   },
   "source": [
    "**Sharding:**\n",
    "Use the `shard` method of the Hugging Face `Dataset` object to split the dataset into 10 smaller pieces, or *shards* (think shards of broken glass). You can read more about sharding at [this link](https://huggingface.co/docs/datasets/en/process#shard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9222e8-427c-472c-9be3-d637e708ec77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "be9222e8-427c-472c-9be3-d637e708ec77",
    "outputId": "ce9d10d8-778b-4637-85e3-83f093ec0585"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into 10 shards and use only the first shard\n",
    "num_shards = 10\n",
    "# Access the 'train' split of the dataset\n",
    "train_dataset = dataset['train']\n",
    "each_shard_len = int(train_dataset.num_rows / num_shards)\n",
    "# shard/chunk to show\n",
    "index_shard = 4\n",
    "print(f'Original data with {train_dataset.num_rows} rows is split to {num_shards} shards , each with {each_shard_len} rows')\n",
    "# Apply shard to the 'train' split\n",
    "dataset = train_dataset.shard(num_shards=num_shards, index=index_shard)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eaaf4",
   "metadata": {
    "id": "2b7eaaf4"
   },
   "source": [
    "### Tokenization\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GAManJdp4hxk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "GAManJdp4hxk",
    "outputId": "4ff7d979-1147-47e5-c666-7068ecc2a2ca"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "image_path = os.path.join(path_to_scripts,\"images\",\"lesson3_tokenization.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eDZPoz0-FrL_",
   "metadata": {
    "id": "eDZPoz0-FrL_"
   },
   "source": [
    "### 3.2 Loading a Tokenizer\n",
    "use a pre-trained tokenizer from the Hugging Face Transformers library  \n",
    "Note: we set `use_fast=False` because the fast Rust implementation sometimes has issues with very long text samples.  \n",
    "Instead, we'll use the Python implementation and leverage parallel processing through the dataset library's map function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1e279-a00b-495f-be17-c349278f60fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 115,
    "id": "5dc1e279-a00b-495f-be17-c349278f60fa",
    "outputId": "0999a3f0-ab5f-414b-9f54-0462fa4241da"
   },
   "outputs": [],
   "source": [
    "# requires conda install conda-forge::sentencepiece\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"SOLAR-10.7B-v1.0\"\n",
    "# Force forward slashes for Hugging Face compatibility\n",
    "upstage_path = os.path.join(\"upstage\",model_name).replace('\\\\', '/') \n",
    "# download model if not present  locally\n",
    "model_path_or_name = os.path.join(path_to_scripts ,\"models\",model_name) if load_model_from_folder else upstage_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    # # Using the Python implementation instead of Rust\n",
    "    use_fast=False\n",
    ")\n",
    "# Test the tokenizer on a simple sentence\n",
    "tokenizer.tokenize(\"I'm a short sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148df352",
   "metadata": {
    "id": "148df352"
   },
   "source": [
    "Create a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62159f4-9764-4685-9353-1d2555a9fe48",
   "metadata": {
    "height": 353,
    "id": "e62159f4-9764-4685-9353-1d2555a9fe48"
   },
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    # Tokenize the text into tokens\n",
    "    tokens = tokenizer.tokenize(example[\"text\"])\n",
    "\n",
    "    # Convert tokens to numerical IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Add special tokens: beginning-of-sequence (BOS) and\n",
    "    #end-of-sequence (EOS)\n",
    "    # These help the model recognize where sequences start and end\n",
    "    token_ids = [tokenizer.bos_token_id] + token_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "    # Store the token IDs in the example\n",
    "    example[\"input_ids\"] = token_ids\n",
    "\n",
    "    # Count the number of tokens for later analysis\n",
    "    example[\"num_tokens\"] = len(token_ids)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956459d",
   "metadata": {
    "id": "a956459d"
   },
   "source": [
    "###  3.3 Applying Tokenization to the Dataset\n",
    "Let's apply the  tokenization function to the sharded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5581f-003c-44f7-abb7-6b004c53dc5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "5ce5581f-003c-44f7-abb7-6b004c53dc5d",
    "outputId": "333bdce2-2f33-4882-e8e9-a24a28b2006b"
   },
   "outputs": [],
   "source": [
    "# Apply the tokenization function to data\n",
    "dataset = dataset.map(tokenization, load_from_cache_file=True)\n",
    "print(dataset)\n",
    "# Look at one example row\n",
    "sample = dataset[3]\n",
    "# load  First 30 characters of text\n",
    "print(\"text\", sample[\"text\"][:30])\n",
    "  # First 30 token IDs\n",
    "print(\"\\ninput_ids\", sample[\"input_ids\"][:30])\n",
    "print(\"\\nnum_tokens\", sample[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989d139",
   "metadata": {
    "id": "a989d139"
   },
   "source": [
    "###   3.4 Calculating Total Tokens\n",
    "Even with our small shard of data (about 4,000 text samples), we have **millions of tokens.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17587a90-a707-465e-868d-75ea7b647dfb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "17587a90-a707-465e-868d-75ea7b647dfb",
    "outputId": "b7533772-e1a6-4348-f1c8-7d325d8bf7e8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "total_tokens = np.sum(dataset[\"num_tokens\"])\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8b1a8-f9ec-40d4-8908-370062183b2b",
   "metadata": {
    "id": "31f8b1a8-f9ec-40d4-8908-370062183b2b"
   },
   "source": [
    "#### Packing the Data\n",
    "Our dataset currently contains **examples of variable lengths**, but for efficient training, we need fixed-length sequences. We'll now **pack our tokens into uniform-length sequences.**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46VLW3ffGkim",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "46VLW3ffGkim",
    "outputId": "91f16527-f743-4670-e45d-93931a61a2ca"
   },
   "outputs": [],
   "source": [
    "image_path = os.path.join(path_to_scripts,\"images\",\"lesson3_packing.jpg\")\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386325db",
   "metadata": {
    "id": "386325db"
   },
   "source": [
    "\n",
    "### 4.1 Concatenate All Tokens and Set Maximum Sequence Length\n",
    "* Concatenate all token IDs into one long list and\n",
    "* Decide on a **maximum sequence length** for our model:This parameter   \n",
    "   _determines the fixed length of token_ sequences that will be fed into your model during training.\n",
    "\n",
    "**Key Considerations When setting the maximum sequence length**\n",
    "\n",
    "* **Memory constraints**: **Longer sequences require more GPU/TPU memory** during training.\n",
    "\n",
    "* **Model architecture**: Different model architectures support different context lengths. Modern models like SOLAR and Llama 2 typically use 4096 tokens or longer as their maximum sequence length.\n",
    "\n",
    "* **Task requirements**: The nature of the  downstream tasks influences the ideal sequence length. Tasks requiring **long-range understanding (like summarizing books) **benefit from longer sequences.\n",
    "\n",
    "* **Training efficiency**: Shorter sequences allow for **larger batch sizes**, which can speed up training, but at the **cost of potentially limiting the model's ability to capture long-range dependencies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d75e13-9038-4c4b-9bf4-8230260dcc54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "61d75e13-9038-4c4b-9bf4-8230260dcc54",
    "outputId": "900f1951-b97e-4f55-8448-e884355fa007"
   },
   "outputs": [],
   "source": [
    "# Concatenate all input_ids into a single long array\n",
    "input_ids = np.concatenate(dataset[\"input_ids\"])\n",
    "print(f\"Total number of concatenated tokens: {len(input_ids)}\")\n",
    "\n",
    "# Set the maximum sequence length\n",
    "# In practice, modern LLMs like SOLAR and Llama 2 use 4096 or longer\n",
    "max_seq_length = 32  # Using a small value for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OOZQZaHy2GSZ",
   "metadata": {
    "id": "OOZQZaHy2GSZ"
   },
   "source": [
    "\n",
    " ### 4.2 Reshape into Fixed-Length Sequences\n",
    "   Reshape the  token list into fixed-length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZenicB2E1Wwx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZenicB2E1Wwx",
    "outputId": "4f75b2aa-039f-4601-dcf6-8d59482c943e"
   },
   "outputs": [],
   "source": [
    "# Calculate the total length that's divisible by max_seq_length\n",
    "total_length = len(input_ids) - len(input_ids) % max_seq_length\n",
    "print(f\"Adjusted total length: {total_length}\")\n",
    "\n",
    "# Discard extra tokens from the end to make it evenly divisible\n",
    "input_ids = input_ids[:total_length]\n",
    "print(f\"Shape after truncation: {input_ids.shape}\")\n",
    "\n",
    "# Reshape into a 2D array with dimensions [num_sequences, max_seq_length]\n",
    "input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)\n",
    "print(f\"Shape after reshaping: {input_ids_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1N30V8mi2Pf9",
   "metadata": {
    "id": "1N30V8mi2Pf9"
   },
   "source": [
    "\n",
    "### 4.3 Convert to Hugging Face Dataset and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ece0a4-663c-40fb-a468-ca1d0e9b8b15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "e6bb462583414da5b1fc1c922d23548e",
      "4366da0d4a314383ba393b5fa5ffa200",
      "70e293c8315848318202613b41acc7e0",
      "e627680679034fd79abaf33a9e7eb3fc",
      "e3707dc5e1be404f8465f21e258cccbc",
      "4333c259e3df45c6a8bdac38fe2c46e3",
      "42e3f920d1fc41ea86c6b5e0e24386e1",
      "4ab81d7a06044935a65c0d1c17f326d3",
      "980bea517f9d44e383f58a9d3bacbc03",
      "db7c81895ba740f1979675445c93e308",
      "148af808308b492fbe64ffdee886afc9"
     ]
    },
    "height": 30,
    "id": "40ece0a4-663c-40fb-a468-ca1d0e9b8b15",
    "outputId": "5e291541-4647-403b-8677-8e850df42235"
   },
   "outputs": [],
   "source": [
    "# Convert numpy array to list for Dataset creation\n",
    "input_ids_list = input_ids_reshaped.tolist()\n",
    "\n",
    "# Create a new Dataset with the packed sequences\n",
    "packaged_pretrain_dataset = datasets.Dataset.from_dict(\n",
    "    {\"input_ids\": input_ids_list}\n",
    ")\n",
    "print(packaged_pretrain_dataset)\n",
    "# Save the packed dataset as a  json file\n",
    "save_dir = load_dir\n",
    "file_path = save_dir + \".//packaged_pretrained_dataset.json\"\n",
    "packaged_pretrain_dataset.to_json(file_path)\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"File '{file_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' creation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f032e-89d6-45d6-89ff-72008144fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_remote  :\n",
    "  from google.colab import drive\n",
    "  drive.flush_and_unmount()\n",
    "  print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "148af808308b492fbe64ffdee886afc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "42e3f920d1fc41ea86c6b5e0e24386e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "4333c259e3df45c6a8bdac38fe2c46e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4366da0d4a314383ba393b5fa5ffa200": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_4333c259e3df45c6a8bdac38fe2c46e3",
      "placeholder": "​",
      "style": "IPY_MODEL_42e3f920d1fc41ea86c6b5e0e24386e1",
      "tabbable": null,
      "tooltip": null,
      "value": "Creating json from Arrow format: 100%"
     }
    },
    "4ab81d7a06044935a65c0d1c17f326d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70e293c8315848318202613b41acc7e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_4ab81d7a06044935a65c0d1c17f326d3",
      "max": 17,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_980bea517f9d44e383f58a9d3bacbc03",
      "tabbable": null,
      "tooltip": null,
      "value": 17
     }
    },
    "980bea517f9d44e383f58a9d3bacbc03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "db7c81895ba740f1979675445c93e308": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3707dc5e1be404f8465f21e258cccbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e627680679034fd79abaf33a9e7eb3fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_db7c81895ba740f1979675445c93e308",
      "placeholder": "​",
      "style": "IPY_MODEL_148af808308b492fbe64ffdee886afc9",
      "tabbable": null,
      "tooltip": null,
      "value": " 17/17 [00:00&lt;00:00, 173.03ba/s]"
     }
    },
    "e6bb462583414da5b1fc1c922d23548e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4366da0d4a314383ba393b5fa5ffa200",
       "IPY_MODEL_70e293c8315848318202613b41acc7e0",
       "IPY_MODEL_e627680679034fd79abaf33a9e7eb3fc"
      ],
      "layout": "IPY_MODEL_e3707dc5e1be404f8465f21e258cccbc",
      "tabbable": null,
      "tooltip": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
